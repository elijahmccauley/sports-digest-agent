{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e4a7f4a3",
      "metadata": {},
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "# NYU Agentic AI Workshop - Session 1\n",
        "\n",
        "## Introduction to Agentic AI & Basic Tools\n",
        "\n",
        "![HiAdi](https://github.com/adityaarunsinghal/agentic-ai-workshop-2025/blob/main/notebooks/media/hi_adi.gif?raw=true)\n",
        "\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76b76137",
      "metadata": {},
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "## About Your Instructor\n",
        "\n",
        "<img src=\"https://github.com/adityaarunsinghal/agentic-ai-workshop-2025/blob/main/notebooks/media/amazon_bedrock_logo.png?raw=true\" width=\"400\" style=\"display: inline-block; margin: 10px;\" alt=\"AmazonBedrockLogo\">\n",
        "<img src=\"https://github.com/adityaarunsinghal/agentic-ai-workshop-2025/blob/main/notebooks/media/amazon_q.png?raw=true\" width=\"400\" style=\"display: inline-block; margin: 10px;\" alt=\"amazonq\">\n",
        "\n",
        "### I work here; But other than that—no relation to the workshop or its contents!\n",
        "\n",
        "_This workshop uses OpenRouter and open-source tools_\n",
        "\n",
        "Visit https://openrouter.ai/redeem and enter code `<redacted>`\n",
        "\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39628128",
      "metadata": {},
      "source": [
        "---\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "<br/>\n",
        "\n",
        "## Who You Are (Based on Survey Results)\n",
        "\n",
        "<img src=\"https://github.com/adityaarunsinghal/agentic-ai-workshop-2025/blob/main/notebooks/media/segments.png?raw=true\" width=\"800\" alt=\"segments\">\n",
        "\n",
        "<img src=\"https://github.com/adityaarunsinghal/agentic-ai-workshop-2025/blob/main/notebooks/media/overview_agentic_experience.png?raw=true\" width=\"800\" alt=\"overview_agentic_experience\">\n",
        "\n",
        "<img src=\"https://github.com/adityaarunsinghal/agentic-ai-workshop-2025/blob/main/notebooks/media/overview_python_experience.png?raw=true\" width=\"800\" alt=\"overview_python_experience\">\n",
        "\n",
        "<img src=\"https://github.com/adityaarunsinghal/agentic-ai-workshop-2025/blob/main/notebooks/media/overview_program_status.png?raw=true\" width=\"800\" alt=\"overview_program_status\">\n",
        "\n",
        "<img src=\"https://github.com/adityaarunsinghal/agentic-ai-workshop-2025/blob/main/notebooks/media/enthusiasm_and_depth.png?raw=true\" width=\"800\" alt=\"enthusiasm_and_depth\">\n",
        "\n",
        "**Mixed experience levels** → Perfect for peer learning\n",
        "\n",
        "**2 or so hours/week for your own practice** → Use the openrouter keys, get more out of the workshop with discussions\n",
        "\n",
        "**Too technical + not technical enough** → Come to office hours! Link on website\n",
        "\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d06272c5",
      "metadata": {},
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "## The \"Agentic AI\" Hype\n",
        "\n",
        "### Let's check the buzz at the bottom of the [Workshop Website](http://adityasinghal.com/agentic-ai-workshop)\n",
        "\n",
        "<img src=\"https://github.com/adityaarunsinghal/agentic-ai-workshop-2025/blob/main/notebooks/media/ai_hype.png?raw=true\" width=\"800\" alt=\"ai_hype\">\n",
        "\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0da9915a",
      "metadata": {},
      "source": [
        "---\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "<br/>\n",
        "\n",
        "# The Evolution of AI Capabilities\n",
        "\n",
        "## Era 1: \"Wow, it talks back\" - January 2021\n",
        "\n",
        "<img src=\"https://github.com/adityaarunsinghal/agentic-ai-workshop-2025/blob/main/notebooks/media/early_days.png?raw=true\" width=\"800\" alt=\"early_days\">\n",
        "\n",
        "(Pretty sure this was RoBERTa) Revolutionary but often nonsensical. We were amazed it could complete sentences!\n",
        "\n",
        "## Era 2: \"Wow, it makes sense\" - December 2022\n",
        "\n",
        "<img src=\"https://github.com/adityaarunsinghal/agentic-ai-workshop-2025/blob/main/notebooks/media/chatgpt_early_free_dec22.png?raw=true\" width=\"800\" alt=\"chatgpt_early_free_dec22\">\n",
        "\n",
        "ChatGPT changed everything. Coherent, helpful conversations. This is what most of you have used, since.\n",
        "\n",
        "## Era 3: \"Wow, it 'perceives'\" - March 2023\n",
        "\n",
        "<img src=\"https://github.com/adityaarunsinghal/agentic-ai-workshop-2025/blob/main/notebooks/media/chatgpt_multimodal_march23.png?raw=true\" width=\"800\" alt=\"chatgpt_multimodal_march23\">\n",
        "\n",
        "Multimodal capabilities: Now it could see images, understand charts, process documents.\n",
        "\n",
        "## Era 4: \"Wow, it can take actions\" - May 2024\n",
        "\n",
        "<img src=\"https://github.com/adityaarunsinghal/agentic-ai-workshop-2025/blob/main/notebooks/media/claude_tool_use_may24.png?raw=true\" width=\"800\" alt=\"claude_tool_use_may24\">\n",
        "\n",
        "### 🎯 THIS is where things get AGENTIC!\n",
        "\n",
        "Not just talking, but DOING. From understanding to action. This is our focus.\n",
        "\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tutorial_begins",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Let's Start Coding!\n",
        "\n",
        "## First: Basic LLM Calls\n",
        "\n",
        "We'll build from the ground up, starting with simple LLM invocation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "68699b81",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Environment ready!\n",
            "📍 Using model: anthropic/claude-3.5-haiku\n",
            "🔑 API key loaded: Yes\n"
          ]
        }
      ],
      "source": [
        "# Environment Setup\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "import asyncio\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Optional, Any\n",
        "\n",
        "# Load environment variables\n",
        "import dotenv\n",
        "\n",
        "dotenv.load_dotenv()\n",
        "\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "# For making HTTP requests\n",
        "import httpx\n",
        "\n",
        "# API Configuration\n",
        "API_URL = \"https://openrouter.ai/api/v1/chat/completions\"\n",
        "HEADERS = {\n",
        "    \"Authorization\": f\"Bearer {os.getenv('OPENROUTER_API_KEY')}\",\n",
        "    \"Content-Type\": \"application/json\",\n",
        "}\n",
        "DEFAULT_MODEL = \"anthropic/claude-3.5-haiku\"\n",
        "DEFAULT_TEMPERATURE = 0.7\n",
        "DEFAULT_MAX_TOKENS = 2048\n",
        "TIMEOUT = 60.0\n",
        "\n",
        "print(\"✅ Environment ready!\")\n",
        "print(f\"📍 Using model: {DEFAULT_MODEL}\")\n",
        "print(f\"🔑 API key loaded: {'Yes' if os.getenv('OPENROUTER_API_KEY') else 'No'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "llm_anatomy",
      "metadata": {},
      "source": [
        "## The Anatomy of an LLM Call\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "basic_llm_functions",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ LLM functions defined!\n"
          ]
        }
      ],
      "source": [
        "def invokeModel(prompt: str, model: str = DEFAULT_MODEL) -> str:\n",
        "    \"\"\"Simple synchronous LLM invocation.\"\"\"\n",
        "    response = httpx.post(\n",
        "        API_URL,\n",
        "        headers=HEADERS,\n",
        "        json={\"model\": model, \"messages\": [{\"role\": \"user\", \"content\": prompt}]},\n",
        "        timeout=TIMEOUT,\n",
        "    )\n",
        "    return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "\n",
        "def parseSSE(response, handler):\n",
        "    \"\"\"Parse Server-Sent Events for streaming responses.\"\"\"\n",
        "    buffer = \"\"\n",
        "    for chunk in response.iter_text():\n",
        "        buffer += chunk\n",
        "        while \"\\n\" in buffer:\n",
        "            line, buffer = buffer.split(\"\\n\", 1)\n",
        "            if line.startswith(\"data: \"):  # SSE data line\n",
        "                data = line[6:]\n",
        "                if data == \"[DONE]\":\n",
        "                    return\n",
        "                try:\n",
        "                    handler(json.loads(data))\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "\n",
        "def invokeModelStream(prompt: str, model: str = DEFAULT_MODEL):\n",
        "    \"\"\"Stream tokens as they arrive for better UX.\"\"\"\n",
        "    with httpx.stream(\n",
        "        \"POST\",\n",
        "        API_URL,\n",
        "        headers=HEADERS,\n",
        "        json={\n",
        "            \"model\": model,\n",
        "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "            \"stream\": True,\n",
        "            \"temperature\": DEFAULT_TEMPERATURE,\n",
        "            \"max_output_tokens\": DEFAULT_MAX_TOKENS,\n",
        "        },\n",
        "        timeout=TIMEOUT,\n",
        "    ) as response:\n",
        "        parseSSE(\n",
        "            response,\n",
        "            lambda data: (\n",
        "                print(\n",
        "                    data[\"choices\"][0][\"delta\"].get(\"content\", \"\"), end=\"\", flush=True\n",
        "                )\n",
        "                if \"choices\" in data\n",
        "                else None\n",
        "            ),\n",
        "        )\n",
        "\n",
        "\n",
        "print(\"✅ LLM functions defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "time_problem",
      "metadata": {},
      "source": [
        "## The Fundamental Limitation\n",
        "\n",
        "Let's ask the LLM something it can't know:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "failed_time_request",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I apologize, but I cannot actually tell you the current time. As an AI, I do not have real-time access to the current time. To get the exact current time, you would need to:\n",
            "\n",
            "• Check a clock\n",
            "• Look at your device's time display\n",
            "• Use an online time service\n",
            "• Check a watch or phone\n",
            "\n",
            "The time will depend on your specific time zone and location."
          ]
        }
      ],
      "source": [
        "PROMPT = \"What time is it right now? Be specific with hours, minutes, and seconds. Bullet point response\"\n",
        "\n",
        "invokeModelStream(PROMPT)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "enter_tools",
      "metadata": {},
      "source": [
        "## From Chat to Agent: Adding Tools\n",
        "\n",
        "Now let's give the LLM the ability to check the time by calling Python functions!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "tool_functions",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing tools:\n",
            "  - Current time: 04:45:05 PM on Wednesday, October 08, 2025\n",
            "  - Today is Wednesday, October 08, 2025\n"
          ]
        }
      ],
      "source": [
        "# Define tool functions - just regular Python!\n",
        "def get_current_time():\n",
        "    \"\"\"Tool function that returns current time with seconds.\"\"\"\n",
        "    now = datetime.now()\n",
        "    return f\"Current time: {now.strftime('%I:%M:%S %p on %A, %B %d, %Y')}\"\n",
        "\n",
        "\n",
        "def get_current_day():\n",
        "    \"\"\"Tool function that returns current day of the week.\"\"\"\n",
        "    now = datetime.now()\n",
        "    return f\"Today is {now.strftime('%A, %B %d, %Y')}\"\n",
        "\n",
        "\n",
        "# Test our tools locally\n",
        "print(\"Testing tools:\")\n",
        "print(f\"  - {get_current_time()}\")\n",
        "print(f\"  - {get_current_day()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "tool_schemas",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Tool schemas defined!\n",
            "📋 Available tools: get_current_time, get_current_day\n"
          ]
        }
      ],
      "source": [
        "# Define tool schemas in the open router format which uses the OpenAI function calling spec\n",
        "# We need this because we don't want to do any \"smart stuff\" on the model's outputs\n",
        "# let the model do its own smart stuff and give us exact params\n",
        "\n",
        "TIME_TOOL = {\n",
        "    \"type\": \"function\",\n",
        "    \"function\": {\n",
        "        \"name\": \"get_current_time\",\n",
        "        \"description\": \"Get the current time with seconds\",\n",
        "        \"parameters\": {\"type\": \"object\", \"properties\": {}},\n",
        "    },\n",
        "}\n",
        "\n",
        "DAY_TOOL = {\n",
        "    \"type\": \"function\",\n",
        "    \"function\": {\n",
        "        \"name\": \"get_current_day\",\n",
        "        \"description\": \"Get the current day of the week\",\n",
        "        \"parameters\": {\"type\": \"object\", \"properties\": {}},\n",
        "    },\n",
        "}\n",
        "\n",
        "print(\"✅ Tool schemas defined!\")\n",
        "print(\"📋 Available tools: get_current_time, get_current_day\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "tool_calling_implementation",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Tool calling implemented!\n"
          ]
        }
      ],
      "source": [
        "# Implement tool calling with conversation memory\n",
        "\n",
        "conversation_history = []\n",
        "\n",
        "\n",
        "def invokeModelWithTools(prompt, tools=None, model=DEFAULT_MODEL):\n",
        "    \"\"\"Tool calling with streaming final response and conversation memory.\"\"\"\n",
        "    global conversation_history\n",
        "    tools_map = {\n",
        "        \"get_current_time\": get_current_time,\n",
        "        \"get_current_day\": get_current_day,\n",
        "    }\n",
        "\n",
        "    # Add new user message to history\n",
        "    conversation_history.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "    # Step 1: Get tool calls\n",
        "    response = httpx.post(\n",
        "        API_URL,\n",
        "        headers=HEADERS,\n",
        "        json={\"model\": model, \"messages\": conversation_history, \"tools\": tools},\n",
        "        timeout=TIMEOUT,\n",
        "    ).json()\n",
        "\n",
        "    assistant_msg = response[\"choices\"][0][\"message\"]\n",
        "    conversation_history.append(assistant_msg)\n",
        "\n",
        "    # Step 2: Execute tool calls\n",
        "    if assistant_msg.get(\"tool_calls\"):\n",
        "        for tool_call in assistant_msg[\"tool_calls\"]:\n",
        "            tool_name = tool_call[\"function\"][\"name\"]\n",
        "            print(f\"> 🔧 Calling tool: {tool_name}\")\n",
        "            if tool_name in tools_map:\n",
        "                result = tools_map[tool_name]()\n",
        "                conversation_history.append(\n",
        "                    {\"role\": \"tool\", \"tool_call_id\": tool_call[\"id\"], \"content\": result}\n",
        "                )\n",
        "\n",
        "    # Step 3: Stream final response\n",
        "    final_content = \"\"\n",
        "\n",
        "    def capture_content(data):\n",
        "        nonlocal final_content\n",
        "        content = data[\"choices\"][0][\"delta\"].get(\"content\", \"\")\n",
        "        final_content += content\n",
        "        print(content, end=\"\", flush=True)\n",
        "\n",
        "    with httpx.stream(\n",
        "        \"POST\",\n",
        "        API_URL,\n",
        "        headers=HEADERS,\n",
        "        json={\n",
        "            \"model\": model,\n",
        "            \"messages\": conversation_history,\n",
        "            \"tools\": tools,\n",
        "            \"stream\": True,\n",
        "        },\n",
        "        timeout=TIMEOUT,\n",
        "    ) as response:\n",
        "        parseSSE(response, capture_content)\n",
        "\n",
        "    # Add final response to history\n",
        "    conversation_history.append({\"role\": \"assistant\", \"content\": final_content})\n",
        "    print()\n",
        "\n",
        "\n",
        "print(\"✅ Tool calling implemented!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "agent_demo",
      "metadata": {},
      "source": [
        "## Moment of Magic: Our First Agent!\n",
        "\n",
        "Watch as the LLM uses tools to answer questions it couldn't before:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "working_agent",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "🔄 Iteration 1\n",
            "==================================================\n",
            "\n",
            "> 🔧 Calling tool: get_current_time\n",
            "Good afternoon, it's great to see you at exactly 04:45:07 PM! \n",
            "\n",
            "(Note: The greeting is unique and based on the current time, and I've provided the precise time as requested. Each time you ask, I'll generate a different simple greeting to ensure variety.)\n",
            "\n",
            "==================================================\n",
            "🔄 Iteration 2\n",
            "==================================================\n",
            "\n",
            "> 🔧 Calling tool: get_current_time\n",
            "Warmest wishes at this precise moment of 04:45:11 PM!\n",
            "\n",
            "==================================================\n",
            "🔄 Iteration 3\n",
            "==================================================\n",
            "\n",
            "> 🔧 Calling tool: get_current_time\n",
            "Wishing you a pleasant afternoon right now at exactly 04:45:15 PM!\n",
            "\n",
            "💡 Key Observations:\n",
            "  1. Different greetings each time (context awareness)\n",
            "  2. Accurate time that advances (real tool calls) = 'looks at a clock' all by itself'\n",
            "  3. Didn't use day tool (intelligent selection)\n",
            "  4. This is a LOOP that adapts - the essence of agents!\n"
          ]
        }
      ],
      "source": [
        "# Loop to show the agent in action\n",
        "prompt = (\n",
        "    \"Greet me with on sentence based on the time of day and tell me the exact time including seconds. \"\n",
        "    \"Never greet me the same way twice. Keep greetings simple. BE FACTUAL, NEVER LIE\"\n",
        ")\n",
        "\n",
        "for i in range(3):\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"🔄 Iteration {i+1}\")\n",
        "    print(f\"{'='*50}\\n\")\n",
        "\n",
        "    invokeModelWithTools(prompt, [TIME_TOOL, DAY_TOOL])\n",
        "\n",
        "print(\"\\n💡 Key Observations:\")\n",
        "print(\"  1. Different greetings each time (context awareness)\")\n",
        "print(\n",
        "    \"  2. Accurate time that advances (real tool calls) = 'looks at a clock' all by itself'\"\n",
        ")\n",
        "print(\"  3. Didn't use day tool (intelligent selection)\")\n",
        "print(\"  4. This is a LOOP that adapts - the essence of agents!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "integration_problem",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# The Integration Nightmare\n",
        "\n",
        "## Every Platform Wants It Different\n",
        "\n",
        "This works great... until you need to use your tools on different platforms. Let's see the problem:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "weather_function",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🌤️  Testing our weather function:\n",
            "Weather in New York: Light rain, 61°F\n"
          ]
        }
      ],
      "source": [
        "# A simple weather function - our core business logic\n",
        "def fetch_weather_data(city: str) -> str:\n",
        "    \"\"\"The actual weather fetching logic - identical everywhere.\"\"\"\n",
        "    # Using wttr.in for simple weather data\n",
        "    try:\n",
        "        response = httpx.get(f\"http://wttr.in/{city}?format=j1\", timeout=5.0)\n",
        "        data = response.json()\n",
        "        current = data[\"current_condition\"][0]\n",
        "        return f\"Weather in {city}: {current['weatherDesc'][0]['value']}, {current['temp_F']}°F\"\n",
        "    except:\n",
        "        return f\"Could not get weather for {city}\"\n",
        "\n",
        "\n",
        "# Test it\n",
        "print(\"🌤️  Testing our weather function:\")\n",
        "print(fetch_weather_data(\"New York\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "platform_chaos",
      "metadata": {},
      "source": [
        "## The Same Tool, Three Different Wrappers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "platform_wrappers",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "THREE DIFFERENT WAYS TO DO THE SAME THING!\n",
            "\n",
            "\n",
            "💭 Imagine maintaining this across 10 tools and 5 platforms...\n",
            "That's 50 different integrations to keep in sync!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 1. OpenAI Function Calling Format\n",
        "def openai_weather_integration():\n",
        "    return [\n",
        "        {\n",
        "            \"type\": \"function\",\n",
        "            \"function\": {\n",
        "                \"name\": \"get_weather\",\n",
        "                \"description\": \"Get current weather for a city\",\n",
        "                \"parameters\": {\n",
        "                    \"type\": \"object\",\n",
        "                    \"properties\": {\n",
        "                        \"city\": {\"type\": \"string\", \"description\": \"City name\"}\n",
        "                    },\n",
        "                    \"required\": [\"city\"],\n",
        "                },\n",
        "            },\n",
        "        }\n",
        "    ]\n",
        "\n",
        "\n",
        "# 2. Anthropic Tool Use Format\n",
        "def anthropic_weather_integration():\n",
        "    return [\n",
        "        {\n",
        "            \"name\": \"get_weather\",\n",
        "            \"description\": \"Get current weather for a city\",\n",
        "            \"input_schema\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\"city\": {\"type\": \"string\", \"description\": \"City name\"}},\n",
        "                \"required\": [\"city\"],\n",
        "            },\n",
        "        }\n",
        "    ]\n",
        "\n",
        "\n",
        "# 3. LangChain/Custom Framework Format\n",
        "class CustomWeatherTool:\n",
        "    def __init__(self):\n",
        "        self.name = \"get_weather\"\n",
        "        self.description = \"Get current weather for a city\"\n",
        "\n",
        "    def get_schema(self):\n",
        "        return {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\"city\": {\"type\": \"string\"}},\n",
        "            \"required\": [\"city\"],\n",
        "        }\n",
        "\n",
        "    def invoke(self, city: str) -> str:\n",
        "        return fetch_weather_data(city)\n",
        "\n",
        "\n",
        "print(\"THREE DIFFERENT WAYS TO DO THE SAME THING!\\n\")\n",
        "print(\"\\n💭 Imagine maintaining this across 10 tools and 5 platforms...\")\n",
        "print(\"That's 50 different integrations to keep in sync!\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ac6323a",
      "metadata": {},
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "# Wanting to share a charger with a friend on a road trip\n",
        "\n",
        "## (I've been an Android user all my life)\n",
        "\n",
        "<img src=\"https://github.com/adityaarunsinghal/agentic-ai-workshop-2025/blob/main/notebooks/media/apple_chargers_evolution.jpg?raw=true\" width=\"600\" alt=\"apple_chargers_evolution\">\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mcp_solution",
      "metadata": {},
      "source": [
        "---\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
        "<br/>\n",
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "# Enter MCP: The USB-C of AI Tools\n",
        "\n",
        "## One Standard, Many Clients\n",
        "\n",
        "Model Context Protocol (MCP) solves this by creating a single standard that all AI clients can use.\n",
        "\n",
        "</div>\n",
        "\n",
        "<br/>\n",
        "\n",
        "<div style=\"border-left: 4px solid #0066cc; padding-left: 20px; margin: 20px 0;\">\n",
        "\n",
        "### After MCP:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"servers\": {\n",
        "    \"weather-mcp-server\": {\n",
        "      \"command\": \"python\",\n",
        "      \"args\": [\"weather-server.py\"]\n",
        "    }\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "**Same config works in:**\n",
        "\n",
        "- ✅ GitHub Copilot\n",
        "- ✅ Cursor\n",
        "- ✅ Claude Desktop\n",
        "- ✅ FastAgent\n",
        "- ✅ Any MCP-compatible client!\n",
        "\n",
        "It's like USB-C: One port design, works with a vape pen and also a laptop\n",
        "\n",
        "But also MCP != tool use just like USB-C != charging\n",
        "\n",
        "</div>\n",
        "\n",
        "<br/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "996add89",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File created successfully at: ../src/server/weather_server.py\n"
          ]
        }
      ],
      "source": [
        "file_path = \"../src/server/weather_server.py\"\n",
        "os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
        "\n",
        "# The Python code you want to write to the file\n",
        "python_code = \"\"\"#!/usr/bin/env python3\n",
        "\n",
        "import requests\n",
        "from fastmcp import FastMCP\n",
        "from fastmcp.utilities.logging import get_logger\n",
        "\n",
        "# Initialize logger for server lifecycle events\n",
        "logger = get_logger(__name__)\n",
        "\n",
        "\n",
        "# ============= MCP SERVER INITIALIZATION =============\n",
        "\n",
        "# Create FastMCP instance with comprehensive configuration\n",
        "mcp = FastMCP(\n",
        "    name=\"weather-mcp-server\",\n",
        "    instructions=\"You are a weather assistant\",\n",
        ")\n",
        "\n",
        "\n",
        "@mcp.tool()\n",
        "def fetch_weather_data(city: str) -> str:\n",
        "    url = f\"http://wttr.in/{city}?format=j1\"\n",
        "    try:\n",
        "        response = requests.get(url, timeout=5)\n",
        "        data = response.json()\n",
        "        current = data[\"current_condition\"][0]\n",
        "        return f\"Weather in {city}: {current['weatherDesc'][0]['value']}, {current['temp_F']}°F\"\n",
        "    except Exception:\n",
        "        return f\"Could not get weather for {city}\"\n",
        "\n",
        "\n",
        "# ============= SERVER ENTRY POINT =============\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    mcp.run()\n",
        "\"\"\"\n",
        "\n",
        "with open(file_path, \"w\") as f:\n",
        "    f.write(python_code)\n",
        "\n",
        "print(f\"File created successfully at: {file_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "restaurant_analogy",
      "metadata": {},
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "## The Restaurant Analogy\n",
        "\n",
        "Host = Restaurant Manager: The notebook cell, CoPilot\n",
        "\n",
        "Client = Waiters: The MCP client that copilot has implemented following specs and using public MCP SDKs\n",
        "\n",
        "Server = Kitchen: What we made via FastMCP which made following specs easy (FastMCP now ships along with the official python MCP SDK too)\n",
        "\n",
        "LLM = Newly hired Head Chef (Claude!)\n",
        "\n",
        "<img src=\"https://github.com/adityaarunsinghal/agentic-ai-workshop-2025/blob/main/notebooks/media/mcp_restaurant_00_analogy.png?raw=true\" width=\"800\" alt=\"mcp_restaurant_00_analogy\">\n",
        "\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef0c0f9f",
      "metadata": {},
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "<img src=\"https://github.com/adityaarunsinghal/agentic-ai-workshop-2025/blob/main/notebooks/media/mcp_restaurant_01_mcp_host.png?raw=true\" width=\"500\" alt=\"mcp_restaurant_01_mcp_host\">\n",
        "<!-- <img src=\"media/mcp_restaurant_01_mcp_host.png\" width=\"500\" alt=\"mcp_restaurant_01_mcp_host\"> -->\n",
        "\n",
        "<img src=\"https://github.com/adityaarunsinghal/agentic-ai-workshop-2025/blob/main/notebooks/media/mcp_restaurant_02_mcp_clients.png?raw=true\" width=\"500\" alt=\"mcp_restaurant_02_mcp_clients\">\n",
        "<!-- <img src=\"media/mcp_restaurant_02_mcp_clients.png\" width=\"500\" alt=\"mcp_restaurant_02_mcp_clients\"> -->\n",
        "\n",
        "<img src=\"https://github.com/adityaarunsinghal/agentic-ai-workshop-2025/blob/main/notebooks/media/mcp_restaurant_03_mcp_servers.png?raw=true\" width=\"500\" alt=\"mcp_restaurant_03_mcp_servers\">\n",
        "<!-- <img src=\"media/mcp_restaurant_03_mcp_servers.png\" width=\"500\" alt=\"mcp_restaurant_03_mcp_servers\"> -->\n",
        "\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9493c5ab",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "## Let's build something!\n",
        "\n",
        "### Vision - Email response agent!\n",
        "\n",
        "<div style=\"display: inline-block; text-align: left; border-left: 4px solid #0066cc; padding-left: 20px; margin: 20px 0;\">\n",
        "\n",
        "```python\n",
        "# MCP Server - Email Tools\n",
        "@mcp_tool\n",
        "def read_emails(folder=\"inbox\", limit=10):\n",
        "    return email_client.fetch_unread(folder, limit)\n",
        "\n",
        "@mcp_tool\n",
        "def draft_response(email_id, response_text):\n",
        "    return email_client.create_draft(email_id, response_text)\n",
        "\n",
        "@mcp_tool\n",
        "def send_email(draft_id):\n",
        "    return email_client.send(draft_id)\n",
        "\n",
        "\n",
        "# Agent Loop\n",
        "tools = [read_emails, draft_response, send_email]\n",
        "conversation_history = []\n",
        "\n",
        "while True:\n",
        "    # LLM decides what to do\n",
        "    response = llm.chat(\n",
        "        messages=conversation_history,\n",
        "        tools=tools,\n",
        "        system=\"You are an email assistant. Read emails and draft responses.\"\n",
        "    )\n",
        "\n",
        "    if response.tool_calls:\n",
        "        for tool_call in response.tool_calls:\n",
        "            result = mcp.execute_tool(tool_call)\n",
        "            conversation_history.append(result)\n",
        "\n",
        "    sleep(300)\n",
        "```\n",
        "\n",
        "</div>\n",
        "</div>\n",
        "\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "50de6b23",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ec3166535b6b4104a05b29a7f00525a3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='🤖 AGENT FIT OR FLOP? 🤖', style=LabelStyle(font_size='20px', font_weight='bold')), …"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from ipywidgets import Button, Label, VBox, HBox\n",
        "from IPython.display import display\n",
        "\n",
        "cases = [\n",
        "    (\n",
        "        \"H&M virtual shopping assistant for customer queries\",\n",
        "        \"YES\",\n",
        "        \"Strong fit! Supposedly led to 70% query resolution, acceptable errors, requires emphasis on different tools per session\",\n",
        "    ),\n",
        "    (\n",
        "        \"JPMorgan legal contract data extraction\",\n",
        "        \"YES\",\n",
        "        \"Excellent! Supposedly saved 360K hours annually, requires data correlation from diff sources\",\n",
        "    ),\n",
        "    (\n",
        "        \"Bank real-time fraud detection\",\n",
        "        \"NO\",\n",
        "        \"Too slow! Need rule-based for ultra-low latency\",\n",
        "    ),\n",
        "    (\n",
        "        \"Hospital clinical support analyzing medical literature\",\n",
        "        \"Maybe??\",\n",
        "        \"Implies human oversight for high-value decisions\",\n",
        "    ),\n",
        "    (\n",
        "        \"Autonomous loan approvals without human review\",\n",
        "        \"NO\",\n",
        "        \"Dangerous! High-stakes + bias risks need human-in-loop\",\n",
        "    ),\n",
        "    (\n",
        "        \"IBM IT incident correlation across infrastructure\",\n",
        "        \"YES\",\n",
        "        \"Perfect! Supposedly 60% faster resolution, high complexity\",\n",
        "    ),\n",
        "    (\n",
        "        \"Nuclear plant autonomous reactor safety management\",\n",
        "        \"NO\",\n",
        "        \"Life-critical! Zero error tolerance = human control\",\n",
        "    ),\n",
        "    (\n",
        "        \"Claude/Cursor for code traversal and writing\",\n",
        "        \"YES\",\n",
        "        \"Excellent! Unpredictable tasks, many actions, high value\",\n",
        "    ),\n",
        "]\n",
        "\n",
        "score = 0\n",
        "current = 0\n",
        "\n",
        "title = Label(\n",
        "    value=\"🤖 AGENT FIT OR FLOP? 🤖\", style={\"font_weight\": \"bold\", \"font_size\": \"20px\"}\n",
        ")\n",
        "score_label = Label(value=\"Score: 0/0\", style={\"font_size\": \"14px\"})\n",
        "case_label = Label(value=f\"Case 1/{len(cases)}:\\n{cases[0][0]}\")\n",
        "feedback = Label(value=\"\")\n",
        "\n",
        "yes_btn = Button(\n",
        "    description=\"✅ YES\",\n",
        "    button_style=\"success\",\n",
        "    layout={\"width\": \"150px\", \"height\": \"40px\"},\n",
        ")\n",
        "no_btn = Button(\n",
        "    description=\"❌ NO\",\n",
        "    button_style=\"danger\",\n",
        "    layout={\"width\": \"150px\", \"height\": \"40px\"},\n",
        ")\n",
        "\n",
        "\n",
        "def check(answer):\n",
        "    global score, current\n",
        "    correct, reason = cases[current][1], cases[current][2]\n",
        "\n",
        "    if answer == correct:\n",
        "        feedback.value = f\"✅ CORRECT! {reason}\"\n",
        "        score += 1\n",
        "    else:\n",
        "        feedback.value = f\"❌ WRONG! Answer: {correct} - {reason}\"\n",
        "\n",
        "    current += 1\n",
        "    score_label.value = f\"Score: {score}/{current}\"\n",
        "\n",
        "    if current < len(cases):\n",
        "        case_label.value = f\"Case {current+1}/{len(cases)}:\\n\\n{cases[current][0]}\"\n",
        "    else:\n",
        "        final = (\n",
        "            \"🏆 Agent Expert!\"\n",
        "            if score >= 7\n",
        "            else \"👍 Good!\" if score >= 5 else \"📚 Study more!\"\n",
        "        )\n",
        "        case_label.value = \"\"\n",
        "        feedback.value = f\"🎯 GAME OVER! Final Score: {score}/{len(cases)}\\n\\n{final}\"\n",
        "        yes_btn.disabled = no_btn.disabled = True\n",
        "\n",
        "\n",
        "yes_btn.on_click(lambda b: check(\"YES\"))\n",
        "no_btn.on_click(lambda b: check(\"NO\"))\n",
        "\n",
        "display(\n",
        "    VBox(\n",
        "        [\n",
        "            title,\n",
        "            score_label,\n",
        "            case_label,\n",
        "            HBox([yes_btn, no_btn], layout={\"justify_content\": \"center\"}),\n",
        "            feedback,\n",
        "        ],\n",
        "        layout={\"align_items\": \"center\", \"padding\": \"15px\"},\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "newspaper_vision",
      "metadata": {},
      "source": [
        "---\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
        "<br/>\n",
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "# Our Project: The Personalized Newspaper Agent 📰\n",
        "\n",
        "\n",
        "## The Vision\n",
        "\n",
        "**\"I want a personalized newspaper delivered to me for my 30-minute train commute.\"**\n",
        "\n",
        "-> Aggregates news from multiple sources\n",
        "\n",
        "-> Filters by my interests\n",
        "\n",
        "-> Creates a beautiful, readable format\n",
        "\n",
        "-> Emails it to me\n",
        "\n",
        "Let's build this step by step!\n",
        "\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "building_mcp_server",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<div style=\"border-left: 4px solid #0066cc; padding-left: 20px; margin: 20px 0;\">\n",
        "\n",
        "# Building Our MCP Server\n",
        "\n",
        "## Phase 1: Basic Tools and Discoverability\n",
        "\n",
        "Let's start with some tools for our newspaper agent\n",
        "\n",
        "<img src=\"https://github.com/adityaarunsinghal/agentic-ai-workshop-2025/blob/main/notebooks/media/mcp_restaurant_04_tools.png?raw=true\" width=\"500\" alt=\"mcp_restaurant_04_tools\">\n",
        "\n",
        "<!-- <img src=\"media/mcp_restaurant_04_tools.png\" width=\"500\" alt=\"mcp_restaurant_04_tools\"> -->\n",
        "\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "bad_tool_name",
      "metadata": {},
      "outputs": [],
      "source": [
        "# First, let's import FastMCP\n",
        "from fastmcp import FastMCP, Context\n",
        "from pydantic import BaseModel, Field\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "from src.server.services.http_client import BaseHTTPClient\n",
        "\n",
        "HN_API_BASE = \"https://hacker-news.firebaseio.com/v0\"\n",
        "HN_WEBSITE_BASE = \"https://news.ycombinator.com\"\n",
        "\n",
        "\n",
        "class HackerNewsClient(BaseHTTPClient):\n",
        "    async def get_story_ids(self, endpoint: str, count: int) -> list[int]:\n",
        "        async def _fetch():\n",
        "            async with httpx.AsyncClient(**self.client_config) as client:\n",
        "                response = await client.get(f\"{HN_API_BASE}/{endpoint}.json\")\n",
        "                response.raise_for_status()\n",
        "                return response.json()[:count]  # Limit to requested count\n",
        "\n",
        "        try:\n",
        "            ids = await _fetch()\n",
        "            print(f\"Fetched {len(ids)} story IDs from {endpoint}\")\n",
        "            return ids\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to fetch story IDs from {endpoint}: {e}\")\n",
        "            raise\n",
        "\n",
        "    async def get_item(self, item_id: int) -> Optional[dict]:\n",
        "        async def _fetch():\n",
        "            async with httpx.AsyncClient(**self.client_config) as client:\n",
        "                response = await client.get(f\"{HN_API_BASE}/item/{item_id}.json\")\n",
        "                response.raise_for_status()\n",
        "                return response.json()\n",
        "\n",
        "        try:\n",
        "            item = await _fetch()\n",
        "            if item:\n",
        "                print(f\"Fetched item {item_id}: {item.get('type', 'unknown')}\")\n",
        "            return item\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to fetch item {item_id}: {e}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "# Create our MCP server instance\n",
        "mcp_v1 = FastMCP(\n",
        "    name=\"newspaper-agent-server-v1\",\n",
        ")\n",
        "\n",
        "\n",
        "@mcp_v1.tool()\n",
        "# get hacker news stories\n",
        "async def ghn(count) -> str:\n",
        "    \"\"\"\n",
        "    Fetch top 'count' stories from Hacker News and format as markdown.\"\"\"\n",
        "    if not 1 <= count <= 150:\n",
        "        raise ValueError(\"Count must be between 1 and 150\")\n",
        "\n",
        "    client = HackerNewsClient()\n",
        "\n",
        "    # Fetch story IDs from top stories endpoint\n",
        "    story_ids = await client.get_story_ids(\"topstories\", count)\n",
        "\n",
        "    # Fetch detailed story information\n",
        "    stories = []\n",
        "    for story_id in story_ids:\n",
        "        story = await client.get_item(story_id)\n",
        "        if story and story.get(\"title\"):\n",
        "            stories.append(\n",
        "                {\n",
        "                    \"title\": story[\"title\"],\n",
        "                    \"score\": story.get(\"score\", 0),\n",
        "                    \"url\": story.get(\"url\", f\"{HN_WEBSITE_BASE}/item?id={story_id}\"),\n",
        "                }\n",
        "            )\n",
        "\n",
        "    # Format as markdown with numbered list\n",
        "    result = f\"# Top {len(stories)} Hacker News Stories\\n\\n\"\n",
        "    for i, story in enumerate(stories, 1):\n",
        "        result += f\"{i}. **{story['title']}** ({story['score']} points)\\n\"\n",
        "        result += f\"   {story['url']}\\n\\n\"\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "fixed_tool_name",
      "metadata": {},
      "outputs": [],
      "source": [
        "mcp_v2 = FastMCP(\n",
        "    name=\"newspaper-agent-server-v1\",\n",
        "    instructions=\"Server for fetching top Hacker News stories\",\n",
        ")\n",
        "\n",
        "\n",
        "@mcp_v2.tool()\n",
        "async def fetch_hacker_news_stories(count: int = 5) -> str:\n",
        "    \"\"\"\n",
        "    Fetch top stories from Hacker News with titles, scores, and URLs.\n",
        "\n",
        "    Retrieves the most popular current stories from Hacker News,\n",
        "    formatted as a markdown list with essential metadata.\n",
        "\n",
        "    Args:\n",
        "        count: Number of stories to fetch (1-150)\n",
        "        ctx: FastMCP context (injected automatically)\n",
        "\n",
        "    Returns:\n",
        "        str: Markdown formatted list of top HN stories with titles, scores, and URLs\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If count is outside valid range (1-150)\n",
        "    \"\"\"\n",
        "    # Validate input parameters\n",
        "    if not 1 <= count <= 150:\n",
        "        raise ValueError(\"Count must be between 1 and 150\")\n",
        "\n",
        "    client = HackerNewsClient()\n",
        "\n",
        "    # Fetch story IDs from top stories endpoint\n",
        "    story_ids = await client.get_story_ids(\"topstories\", count)\n",
        "\n",
        "    # Fetch detailed story information\n",
        "    stories = []\n",
        "    for story_id in story_ids:\n",
        "        story = await client.get_item(story_id)\n",
        "        if story and story.get(\"title\"):\n",
        "            stories.append(\n",
        "                {\n",
        "                    \"title\": story[\"title\"],\n",
        "                    \"score\": story.get(\"score\", 0),\n",
        "                    \"url\": story.get(\"url\", f\"{HN_WEBSITE_BASE}/item?id={story_id}\"),\n",
        "                }\n",
        "            )\n",
        "\n",
        "    # Format as markdown with numbered list\n",
        "    result = f\"# Top {len(stories)} Hacker News Stories\\n\\n\"\n",
        "    for i, story in enumerate(stories, 1):\n",
        "        result += f\"{i}. **{story['title']}** ({story['score']} points)\\n\"\n",
        "        result += f\"   {story['url']}\\n\\n\"\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdfce6ca",
      "metadata": {},
      "source": [
        "```bash\n",
        "\n",
        "▎◀ News Agent ────────────────────────────────────────────────[tool request - fetch_hacker_news_stories]\n",
        "\n",
        "{'count': 20}\n",
        "\n",
        "─| fetch_hacke… | fetch |───────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "\n",
        "▎▶ News Agent ──────────────────────────────────────────────────────[tool result - Text Only 2452 chars]\n",
        "\n",
        "# Top 20 Hacker News Stories\n",
        "\n",
        "1. **Claude Sonnet 4.5** (875 points)\n",
        "   https://www.anthropic.com/news/claude-sonnet-4-5\n",
        "\n",
        "2. **California governor signs AI transparency bill into law** (117 points)\n",
        "   https://www.gov.ca.gov/2025/09/29/governor-newsom-signs-sb-53-advancing-californias-world-leading-artificial-intelligence-industry/\n",
        "\n",
        "3. **Claude Code 2.0** (423...\n",
        "\n",
        "\n",
        "▎▶ News Agent ──────────────────────────────────────────────────────[tool result - Text Only 3165 chars]\n",
        "\n",
        "Contents of https://www.anthropic.com/news/claude-sonnet-4-5:\n",
        "Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents.\n",
        "It’s the best model at using computers. And it shows substantial gains in reasoning and math.\n",
        "\n",
        "Code is everywhere. It runs every application, spreadsheet, and software tool you use. Being...\n",
        "\n",
        "▎▶ News Agent ─────────────────────────────────────────────────────────────────────[tool result - ERROR]\n",
        "\n",
        "Failed to fetch https://openai.com/index/buy-it-in-chatgpt/ - status code 403\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e041d74",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### We can't assume clients will use a specified server provided by someone else, plus we are getting rejected from some sites\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "64744ef8",
      "metadata": {},
      "outputs": [],
      "source": [
        "from html_to_markdown import convert_to_markdown\n",
        "from typing import Dict\n",
        "\n",
        "from src.server.config.constants import DEFAULT_HEADERS\n",
        "\n",
        "\n",
        "async def fetch(url):\n",
        "    retries = 2\n",
        "    async with httpx.AsyncClient(headers=DEFAULT_HEADERS, timeout=120) as client:\n",
        "        for attempt in range(1, retries + 1):\n",
        "            try:\n",
        "                resp = await client.get(url)\n",
        "                resp.raise_for_status()\n",
        "                return convert_to_markdown(resp.text)[:5000]\n",
        "            except httpx.HTTPError as exc:\n",
        "                if attempt == retries:\n",
        "                    raise\n",
        "                await asyncio.sleep(0.5 * attempt)\n",
        "\n",
        "\n",
        "@mcp_v2.tool()\n",
        "async def fetch_content(url: str) -> str:\n",
        "    \"\"\"\n",
        "    Fetch HTML from `url` and convert to Markdown.\n",
        "\n",
        "    Args:\n",
        "        url: The target URL.\n",
        "        headers: HTTP headers to spoof a real browser.\n",
        "        timeout: Request timeout in seconds.\n",
        "        retries: Number of retry attempts on failure.\n",
        "\n",
        "    Returns:\n",
        "        Markdown-converted text.\n",
        "\n",
        "    Raises:\n",
        "        httpx.HTTPError: On request failure after retries.\n",
        "    \"\"\"\n",
        "    return await fetch(url)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tool_combination_problem",
      "metadata": {},
      "source": [
        "## Phase 2: The \"Always Together\" Problem\n",
        "\n",
        "Some tools are always called together. Let's see this problem and fix it:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "e9ad453b",
      "metadata": {
        "notebookRunGroups": {
          "groupValue": "1"
        }
      },
      "outputs": [],
      "source": [
        "@mcp_v2.prompt(name=\"Detailed Newspaper Request\")\n",
        "async def ask_for_proper_detailed_newspaper() -> str:\n",
        "    \"\"\"Prompt for quick newspaper request\"\"\"\n",
        "    return \"Please make me a detailed newspaper by pulling 30 HN articles, ranking them all by how interesting they are and then fetching content for them all and summarizing them for me with broad themes at the top.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "separate_tools",
      "metadata": {},
      "outputs": [],
      "source": [
        "mcp_v3 = FastMCP(\n",
        "    name=\"newspaper-agent-server-v1\",\n",
        "    instructions=\"Server for fetching top Hacker News stories\",\n",
        ")\n",
        "\n",
        "\n",
        "@mcp_v3.prompt(name=\"Detailed Newspaper Request\")\n",
        "async def ask_for_proper_detailed_newspaper() -> str:\n",
        "    \"\"\"Prompt for quick newspaper request\"\"\"\n",
        "    return \"Please make me a detailed newspaper by pulling 30 HN articles, ranking them all by how interesting they are and summarizing them for me with broad themes at the top.\"\n",
        "\n",
        "\n",
        "@mcp_v3.tool()\n",
        "async def fetch_hacker_news_stories_and_contents(count: int = 5) -> str:\n",
        "    \"\"\"\n",
        "    Fetch top stories from Hacker News with titles, scores, and URLs.\n",
        "\n",
        "    Retrieves the most popular current stories from Hacker News,\n",
        "    formatted as a markdown list with essential metadata.\n",
        "\n",
        "    Args:\n",
        "        count: Number of stories to fetch (1-150)\n",
        "        ctx: FastMCP context (injected automatically)\n",
        "\n",
        "    Returns:\n",
        "        str: Markdown formatted list of top HN stories with titles, scores, and URLs\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If count is outside valid range (1-150)\n",
        "    \"\"\"\n",
        "    # Validate input parameters\n",
        "    if not 1 <= count <= 150:\n",
        "        raise ValueError(\"Count must be between 1 and 150\")\n",
        "\n",
        "    client = HackerNewsClient()\n",
        "\n",
        "    # Fetch story IDs from top stories endpoint\n",
        "    story_ids = await client.get_story_ids(\"topstories\", count)\n",
        "\n",
        "    # Fetch detailed story information\n",
        "    stories = []\n",
        "    for story_id in story_ids:\n",
        "        story = await client.get_item(story_id)\n",
        "        if story and story.get(\"title\"):\n",
        "            url = story.get(\"url\", f\"{HN_WEBSITE_BASE}/item?id={story_id}\")\n",
        "            content = await fetch(url)\n",
        "            stories.append(\n",
        "                {\n",
        "                    \"title\": story[\"title\"],\n",
        "                    \"score\": story.get(\"score\", 0),\n",
        "                    \"url\": url,\n",
        "                    \"content\": content,\n",
        "                    # \"content\": content[:5000]\n",
        "                }\n",
        "            )\n",
        "\n",
        "    # Format as markdown with numbered list\n",
        "    result = f\"# Top {len(stories)} Hacker News Stories\\n\\n\"\n",
        "    for i, story in enumerate(stories, 1):\n",
        "        result += f\"{i}. **{story['title']}** ({story['score']} points)\\n\"\n",
        "        result += f\"   {story['url']}\\n\\n\"\n",
        "        result += f\"   {story['content']}\\n\\n\"\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d05e611",
      "metadata": {},
      "source": [
        "---\n",
        "```bash\n",
        "🏆 STORY RANKINGS BY INTEREST LEVEL       \n",
        "\n",
        "TIER S: Breakthrough Territory ⭐⭐⭐\n",
        "\n",
        "#1 - Claude Sonnet 4.5 Launch (938 points) Anthropic's latest model claims to be the world's best coding model and strongest for building agents. This represents a potential paradigm shift in AI \n",
        "capabilities, especially for autonomous systems and computer use.                                                         \n",
        "\n",
        "#2 - FCC Accidentally Leaked iPhone Schematics (205 points) A rare peek behind Apple's curtain due to regulatory mishap. The leak potentially exposes company secrets to competitors, highlighting \n",
        "the tension between transparency requirements and trade secrets.\n",
        "```\n",
        "---\n",
        "\n",
        "1. Would have been nice to get progress notifications\n",
        "2. Who decides what's interesting?\n",
        "3. Presentation of the newspaper\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "full_server_setup",
      "metadata": {},
      "source": [
        "## Phase 3: Building the Full MCP Server\n",
        "\n",
        "Now let's build our complete newspaper agent with all the tools we need!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "import_services",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ All Phase 3 imports successful!\n",
            "📁 Working directory: /Users/adi/Documents/GitHub/agentic-ai-workshop-2025/notebooks\n",
            "📁 Server path: /Users/adi/Documents/GitHub/agentic-ai-workshop-2025/src/server\n"
          ]
        }
      ],
      "source": [
        "# Cell: Phase 3 Imports and Setup\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional\n",
        "\n",
        "# Add src to path for imports\n",
        "sys.path.insert(0, str(Path.cwd().parent / \"src\" / \"server\"))\n",
        "\n",
        "# Core MCP imports\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Service imports\n",
        "from services.email_service import EmailService\n",
        "from services.interests_file import InterestsFileService\n",
        "\n",
        "# Config imports\n",
        "from config.settings import get_settings\n",
        "\n",
        "print(\"✅ All Phase 3 imports successful!\")\n",
        "print(f\"📁 Working directory: {Path.cwd()}\")\n",
        "print(f\"📁 Server path: {Path.cwd().parent / 'src' / 'server'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "complete_mcp_server",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Application context manager defined!\n"
          ]
        }
      ],
      "source": [
        "# Cell: Application Context Setup\n",
        "from contextlib import asynccontextmanager\n",
        "from dataclasses import dataclass\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class AppContext:\n",
        "    \"\"\"Application context with all services for newspaper creation.\"\"\"\n",
        "\n",
        "    hn_client: HackerNewsClient\n",
        "    interests_service: InterestsFileService\n",
        "    email_service: EmailService\n",
        "    settings: object\n",
        "\n",
        "\n",
        "@asynccontextmanager\n",
        "async def app_lifespan(mcp: FastMCP):\n",
        "    \"\"\"Initialize all services for the newspaper agent.\"\"\"\n",
        "    print(\"🚀 Starting Newspaper Creation Agent MCP Server\")\n",
        "\n",
        "    # Get settings\n",
        "    settings = get_settings()\n",
        "    settings.data_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Initialize all services\n",
        "    hn_client = HackerNewsClient()\n",
        "    interests_service = InterestsFileService(settings.data_dir)\n",
        "\n",
        "    # Email service configuration\n",
        "    email_service = EmailService(\n",
        "        {\n",
        "            \"server\": \"smtp.gmail.com\",\n",
        "            \"port\": 587,\n",
        "            \"use_tls\": True,\n",
        "            \"username\": \"adityaarunsinghal@gmail.com\",\n",
        "            \"password\": os.getenv(\"MCP_SMTP_PASSWORD\", \"\"),\n",
        "            \"from_email\": \"adityaarunsinghal@gmail.com\",\n",
        "            \"from_name\": \"Newspaper Creation Agent\",\n",
        "        }\n",
        "    )\n",
        "\n",
        "    print(\"✅ All services initialized!\")\n",
        "    print(f\"📧 Email: {email_service.server}:{email_service.port}\")\n",
        "    print(f\"📁 Data: {settings.data_dir}\")\n",
        "\n",
        "    try:\n",
        "        yield AppContext(\n",
        "            hn_client=hn_client,\n",
        "            interests_service=interests_service,\n",
        "            email_service=email_service,\n",
        "            settings=settings,\n",
        "        )\n",
        "    finally:\n",
        "        print(\"👋 Shutting down MCP Server\")\n",
        "\n",
        "\n",
        "print(\"✅ Application context manager defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "register_all_tools",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ All newspaper creation tools registered!\n"
          ]
        }
      ],
      "source": [
        "# Cell: MCP Server with Full Tool Suite\n",
        "\n",
        "mcp = FastMCP(\n",
        "    name=\"newspaper-creation-agent\",\n",
        "    instructions=\"\"\"You are a sophisticated newspaper creation assistant.\n",
        "    \n",
        "    Your capabilities:\n",
        "    - Fetch stories from Hacker News\n",
        "    - Extract full article content from URLs\n",
        "    - Track and manage user interests\n",
        "    - Create structured newspapers with sections\n",
        "    - Add articles to newspaper sections\n",
        "    - Send finished newspapers via email\n",
        "    \n",
        "    Current date: {date}\n",
        "    \"\"\".format(\n",
        "        date=datetime.now().strftime(\"%A, %B %d, %Y\")\n",
        "    ),\n",
        "    lifespan=app_lifespan,\n",
        ")\n",
        "\n",
        "# ============= CORE TOOLS =============\n",
        "\n",
        "\n",
        "@mcp.tool()\n",
        "async def fetch_hn_stories(count: int = 5, ctx: Context = None) -> str:\n",
        "    \"\"\"Fetch top stories from Hacker News.\"\"\"\n",
        "    if not 1 <= count <= 30:\n",
        "        raise ValueError(\"Count must be between 1 and 30\")\n",
        "\n",
        "    client = ctx.request_context.lifespan_context.hn_client\n",
        "    story_ids = await client.get_story_ids(\"topstories\", count)\n",
        "\n",
        "    stories = []\n",
        "    for story_id in story_ids:\n",
        "        story = await client.get_item(story_id)\n",
        "        if story and story.get(\"title\"):\n",
        "            stories.append(\n",
        "                {\n",
        "                    \"id\": story_id,\n",
        "                    \"title\": story[\"title\"],\n",
        "                    \"score\": story.get(\"score\", 0),\n",
        "                    \"url\": story.get(\"url\", f\"{HN_WEBSITE_BASE}/item?id={story_id}\"),\n",
        "                    \"by\": story.get(\"by\", \"unknown\"),\n",
        "                }\n",
        "            )\n",
        "\n",
        "    result = f\"# Top {len(stories)} Hacker News Stories\\n\\n\"\n",
        "    for i, story in enumerate(stories, 1):\n",
        "        result += (\n",
        "            f\"{i}. **{story['title']}** ({story['score']} points by {story['by']})\\n\"\n",
        "        )\n",
        "        result += f\"   {story['url']}\\n\\n\"\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "@mcp.tool()\n",
        "async def fetch_article_content(url: str, ctx: Context = None) -> str:\n",
        "    \"\"\"Fetch and extract clean content from a web URL.\"\"\"\n",
        "    try:\n",
        "        content = (await fetch(url))[:8000]\n",
        "        return f\"# Content from {url}\\n\\n{content}\"\n",
        "    except Exception as e:\n",
        "        return f\"❌ Failed to fetch {url}: {str(e)}\"\n",
        "\n",
        "\n",
        "# ============= NEWSPAPER CREATION TOOLS =============\n",
        "\n",
        "\n",
        "@mcp.tool()\n",
        "async def create_newspaper_draft(\n",
        "    title: str = \"Daily Tech Digest\",\n",
        "    subtitle: str = \"\",\n",
        "    sections: List[str] = None,\n",
        "    ctx: Context = None,\n",
        ") -> str:\n",
        "    \"\"\"Create a new newspaper draft with sections.\"\"\"\n",
        "    if sections is None:\n",
        "        sections = [\"Top Stories\", \"Technology\", \"Analysis\"]\n",
        "\n",
        "    newspaper_id = f\"newspaper_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "\n",
        "    draft = {\n",
        "        \"id\": newspaper_id,\n",
        "        \"title\": title,\n",
        "        \"subtitle\": subtitle,\n",
        "        \"created_at\": datetime.now().isoformat(),\n",
        "        \"status\": \"draft\",\n",
        "        \"sections\": [{\"title\": sec, \"articles\": []} for sec in sections],\n",
        "    }\n",
        "\n",
        "    # Save draft\n",
        "    settings = ctx.request_context.lifespan_context.settings\n",
        "    drafts_dir = Path(settings.data_dir) / \"newspapers\"\n",
        "    drafts_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    draft_file = drafts_dir / f\"{newspaper_id}.json\"\n",
        "    with open(draft_file, \"w\") as f:\n",
        "        json.dump(draft, f, indent=2)\n",
        "\n",
        "    # Save HTML draft\n",
        "    email_service = ctx.request_context.lifespan_context.email_service\n",
        "    html_content = email_service._create_html_version(draft)\n",
        "    html_file = drafts_dir / f\"{newspaper_id}.html\"\n",
        "    with open(html_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(html_content)\n",
        "\n",
        "    result = f\"# 📰 Newspaper Draft Created\\n\\n\"\n",
        "    result += f\"**ID:** {newspaper_id}\\n\"\n",
        "    result += f\"**Title:** {title}\\n\"\n",
        "    result += f\"**Sections:** {', '.join(sections)}\\n\"\n",
        "    result += f\"**Files:** {draft_file}, {html_file}\\n\\n\"\n",
        "    result += f\"*Use add_article_to_newspaper() to add content*\"\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "@mcp.tool()\n",
        "async def add_article_to_newspaper(\n",
        "    newspaper_id: str,\n",
        "    section: str,\n",
        "    title: str,\n",
        "    content: str,\n",
        "    url: str = \"\",\n",
        "    author: str = \"\",\n",
        "    score: int = 10,\n",
        "    ctx: Context = None,\n",
        ") -> str:\n",
        "    \"\"\"Add an article to a newspaper section with importance score (1-100).\"\"\"\n",
        "    settings = ctx.request_context.lifespan_context.settings\n",
        "    draft_file = Path(settings.data_dir) / \"newspapers\" / f\"{newspaper_id}.json\"\n",
        "\n",
        "    if not draft_file.exists():\n",
        "        return f\"❌ Newspaper '{newspaper_id}' not found\"\n",
        "\n",
        "    # Load draft\n",
        "    with open(draft_file, \"r\") as f:\n",
        "        draft = json.load(f)\n",
        "\n",
        "    # Find section\n",
        "    target_section = None\n",
        "    for sec in draft[\"sections\"]:\n",
        "        if sec[\"title\"].lower() == section.lower():\n",
        "            target_section = sec\n",
        "            break\n",
        "\n",
        "    if not target_section:\n",
        "        sections = [s[\"title\"] for s in draft[\"sections\"]]\n",
        "        return f\"❌ Section '{section}' not found. Available: {', '.join(sections)}\"\n",
        "\n",
        "    # Add article\n",
        "    article = {\n",
        "        \"title\": title,\n",
        "        \"content\": content,\n",
        "        \"url\": url,\n",
        "        \"author\": author,\n",
        "        \"score\": score,\n",
        "        \"added_at\": datetime.now().isoformat(),\n",
        "    }\n",
        "    target_section[\"articles\"].append(article)\n",
        "\n",
        "    # Save JSON\n",
        "    with open(draft_file, \"w\") as f:\n",
        "        json.dump(draft, f, indent=2)\n",
        "\n",
        "    # Update HTML\n",
        "    email_service = ctx.request_context.lifespan_context.email_service\n",
        "    html_content = email_service._create_html_version(draft)\n",
        "    html_file = draft_file.with_suffix(\".html\")\n",
        "    with open(html_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(html_content)\n",
        "\n",
        "    total_articles = sum(len(s[\"articles\"]) for s in draft[\"sections\"])\n",
        "\n",
        "    return f\"✅ Added '{title}' to {section}\\nTotal articles: {total_articles}\"\n",
        "\n",
        "\n",
        "@mcp.tool()\n",
        "async def send_newspaper_email(newspaper_id: str, ctx: Context = None) -> str:\n",
        "    \"\"\"Send the finished newspaper via email.\"\"\"\n",
        "    settings = ctx.request_context.lifespan_context.settings\n",
        "    draft_file = Path(settings.data_dir) / \"newspapers\" / f\"{newspaper_id}.json\"\n",
        "\n",
        "    if not draft_file.exists():\n",
        "        return f\"❌ Newspaper '{newspaper_id}' not found\"\n",
        "\n",
        "    # Load draft\n",
        "    with open(draft_file, \"r\") as f:\n",
        "        draft = json.load(f)\n",
        "\n",
        "    # Get email service\n",
        "    email_service = ctx.request_context.lifespan_context.email_service\n",
        "\n",
        "    # Send email\n",
        "    result = email_service.send_newspaper(\n",
        "        newspaper_data=draft, subject=f\"📰 {draft['title']}\"\n",
        "    )\n",
        "\n",
        "    if result.get(\"success\"):\n",
        "        return f\"✅ Newspaper '{draft['title']}' sent successfully!\\n\\n{result.get('message', '')}\"\n",
        "    else:\n",
        "        return f\"❌ Failed to send: {result.get('error', 'Unknown error')}\"\n",
        "\n",
        "\n",
        "@mcp.tool()\n",
        "async def preview_newspaper(newspaper_id: str, ctx: Context = None) -> str:\n",
        "    \"\"\"Preview newspaper before sending.\"\"\"\n",
        "    settings = ctx.request_context.lifespan_context.settings\n",
        "    draft_file = Path(settings.data_dir) / \"newspapers\" / f\"{newspaper_id}.json\"\n",
        "\n",
        "    if not draft_file.exists():\n",
        "        return f\"❌ Newspaper '{newspaper_id}' not found\"\n",
        "\n",
        "    with open(draft_file, \"r\") as f:\n",
        "        draft = json.load(f)\n",
        "\n",
        "    result = f\"# 📰 {draft['title']}\\n\"\n",
        "    if draft.get(\"subtitle\"):\n",
        "        result += f\"*{draft['subtitle']}*\\n\"\n",
        "    result += f\"\\n{datetime.now().strftime('%A, %B %d, %Y')}\\n\\n\"\n",
        "\n",
        "    for section in draft[\"sections\"]:\n",
        "        if not section[\"articles\"]:\n",
        "            continue\n",
        "\n",
        "        result += f\"## {section['title']}\\n\\n\"\n",
        "        for i, article in enumerate(section[\"articles\"], 1):\n",
        "            result += f\"### {i}. {article['title']}\\n\"\n",
        "            if article.get(\"author\"):\n",
        "                result += f\"*By {article['author']}*\\n\\n\"\n",
        "\n",
        "            # Preview first 200 chars\n",
        "            preview = article[\"content\"][:200]\n",
        "            if len(article[\"content\"]) > 200:\n",
        "                preview += \"...\"\n",
        "            result += f\"{preview}\\n\\n\"\n",
        "\n",
        "            if article.get(\"url\"):\n",
        "                result += f\"[Read more]({article['url']})\\n\\n\"\n",
        "            result += \"---\\n\\n\"\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "print(\"✅ All newspaper creation tools registered!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2994e46",
      "metadata": {},
      "source": [
        "### Checking articles on: https://mail.google.com/mail/u/0/#search/%22Generated+by+Newspaper+Creation+Agent%22"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "d684f944",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Resources and prompts registered!\n"
          ]
        }
      ],
      "source": [
        "# Cell: Resources and Prompts\n",
        "@mcp.resource(\"file://interests.md\")\n",
        "async def get_interests_file(ctx: Context = None) -> str:\n",
        "    \"\"\"User's interests file.\"\"\"\n",
        "    interests_service = ctx.request_context.lifespan_context.interests_service\n",
        "    interests = interests_service.read_interests()\n",
        "\n",
        "    result = \"# Your Interests\\n\\n\"\n",
        "    result += f\"**Last Updated:** {interests.get('last_updated', 'Unknown')}\\n\\n\"\n",
        "\n",
        "    topics = interests.get(\"topics\", [])\n",
        "    if topics:\n",
        "        result += \"## Topics\\n\"\n",
        "        for topic in topics:\n",
        "            result += f\"- {topic}\\n\"\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "@mcp.prompt()\n",
        "async def create_quick_newspaper() -> str:\n",
        "    \"\"\"Workflow for creating a quick 5-article newspaper.\"\"\"\n",
        "    return \"\"\"Create a quick newspaper with these steps:\n",
        "\n",
        "1. fetch_hn_stories(count=10) - Get recent stories\n",
        "2. create_newspaper_draft(title=\"Tech Brief\", sections=[\"Headlines\", \"Deep Dive\"])\n",
        "3. For 2-3 interesting stories:\n",
        "   - fetch_article_content(url) to get full content\n",
        "   - add_article_to_newspaper() with summary\n",
        "4. preview_newspaper() to review\n",
        "5. send_newspaper_email() to deliver\n",
        "\n",
        "Keep it concise and focused!\"\"\"\n",
        "\n",
        "\n",
        "@mcp.prompt()\n",
        "async def create_comprehensive_newspaper() -> str:\n",
        "    \"\"\"Workflow for creating a detailed newspaper.\"\"\"\n",
        "    return \"\"\"Create a comprehensive newspaper:\n",
        "\n",
        "1. fetch_hn_stories(count=20) - Get more options\n",
        "2. create_newspaper_draft(\n",
        "    title=\"Daily Tech Digest\",\n",
        "    sections=[\"Top Stories\", \"Technology\", \"Analysis\", \"Opinion\"]\n",
        "   )\n",
        "3. For 8-10 stories across different sections:\n",
        "   - fetch_article_content(url)\n",
        "   - Summarize intelligently\n",
        "   - add_article_to_newspaper() to appropriate section\n",
        "4. preview_newspaper() for final review\n",
        "5. send_newspaper_email() to deliver\n",
        "\n",
        "Aim for depth and variety!\"\"\"\n",
        "\n",
        "\n",
        "print(\"✅ Resources and prompts registered!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "dcd2d10f",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "\n",
              "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">╭────────────────────────────────────────────────────────────────────────────╮</span>\n",
              "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>                                                                            <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>\n",
              "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>    <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">    _ __ ___  _____           __  __  _____________    ____    ____ </span>    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>\n",
              "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>    <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">   _ __ ___ .'____/___ ______/ /_/  |/  / ____/ __ \\  |___ \\  / __ \\</span>    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>\n",
              "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>    <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">  _ __ ___ / /_  / __ `/ ___/ __/ /|_/ / /   / /_/ /  ___/ / / / / /</span>    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>\n",
              "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>    <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\"> _ __ ___ / __/ / /_/ (__  ) /_/ /  / / /___/ ____/  /  __/_/ /_/ / </span>    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>\n",
              "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>    <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">_ __ ___ /_/    \\____/____/\\__/_/  /_/\\____/_/      /_____(*)____/  </span>    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>\n",
              "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>                                                                            <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>\n",
              "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>                                                                            <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>\n",
              "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>                                <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">FastMCP  2.0</span>                                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>\n",
              "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>                                                                            <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>\n",
              "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>                                                                            <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>\n",
              "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>               <span style=\"font-weight: bold\">🖥️  </span><span style=\"color: #008080; text-decoration-color: #008080\">Server name:     </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">newspaper-creation-agent </span>                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>\n",
              "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>               <span style=\"font-weight: bold\">📦 </span><span style=\"color: #008080; text-decoration-color: #008080\">Transport:       </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Streamable-HTTP          </span>                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>\n",
              "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>               <span style=\"font-weight: bold\">🔗 </span><span style=\"color: #008080; text-decoration-color: #008080\">Server URL:      </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">http://127.0.0.1:8080/mcp</span>                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>\n",
              "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>               <span style=\"font-weight: bold\">   </span><span style=\"color: #008080; text-decoration-color: #008080\">                 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                         </span>                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>\n",
              "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>               <span style=\"font-weight: bold\">🏎️  </span><span style=\"color: #008080; text-decoration-color: #008080\">FastMCP version: </span><span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">2.12.4                   </span>                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>\n",
              "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>               <span style=\"font-weight: bold\">🤝 </span><span style=\"color: #008080; text-decoration-color: #008080\">MCP SDK version: </span><span style=\"color: #dfdfdf; text-decoration-color: #dfdfdf\">1.16.0                   </span>                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>\n",
              "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>               <span style=\"font-weight: bold\">   </span><span style=\"color: #008080; text-decoration-color: #008080\">                 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                         </span>                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>\n",
              "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>               <span style=\"font-weight: bold\">📚 </span><span style=\"color: #008080; text-decoration-color: #008080\">Docs:            </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">https://gofastmcp.com    </span>                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>\n",
              "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>               <span style=\"font-weight: bold\">🚀 </span><span style=\"color: #008080; text-decoration-color: #008080\">Deploy:          </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">https://fastmcp.cloud    </span>                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>\n",
              "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>                                                                            <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│</span>\n",
              "<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">╰────────────────────────────────────────────────────────────────────────────╯</span>\n",
              "\n",
              "\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n",
              "\n",
              "\u001b[2m╭────────────────────────────────────────────────────────────────────────────╮\u001b[0m\n",
              "\u001b[2m│\u001b[0m                                                                            \u001b[2m│\u001b[0m\n",
              "\u001b[2m│\u001b[0m    \u001b[1;32m    _ __ ___  _____           __  __  _____________    ____    ____ \u001b[0m    \u001b[2m│\u001b[0m\n",
              "\u001b[2m│\u001b[0m    \u001b[1;32m   _ __ ___ .'____/___ ______/ /_/  |/  / ____/ __ \\  |___ \\  / __ \\\u001b[0m    \u001b[2m│\u001b[0m\n",
              "\u001b[2m│\u001b[0m    \u001b[1;32m  _ __ ___ / /_  / __ `/ ___/ __/ /|_/ / /   / /_/ /  ___/ / / / / /\u001b[0m    \u001b[2m│\u001b[0m\n",
              "\u001b[2m│\u001b[0m    \u001b[1;32m _ __ ___ / __/ / /_/ (__  ) /_/ /  / / /___/ ____/  /  __/_/ /_/ / \u001b[0m    \u001b[2m│\u001b[0m\n",
              "\u001b[2m│\u001b[0m    \u001b[1;32m_ __ ___ /_/    \\____/____/\\__/_/  /_/\\____/_/      /_____(*)____/  \u001b[0m    \u001b[2m│\u001b[0m\n",
              "\u001b[2m│\u001b[0m                                                                            \u001b[2m│\u001b[0m\n",
              "\u001b[2m│\u001b[0m                                                                            \u001b[2m│\u001b[0m\n",
              "\u001b[2m│\u001b[0m                                \u001b[1;34mFastMCP  2.0\u001b[0m                                \u001b[2m│\u001b[0m\n",
              "\u001b[2m│\u001b[0m                                                                            \u001b[2m│\u001b[0m\n",
              "\u001b[2m│\u001b[0m                                                                            \u001b[2m│\u001b[0m\n",
              "\u001b[2m│\u001b[0m               \u001b[1m🖥️ \u001b[0m\u001b[1m \u001b[0m\u001b[36mServer name:    \u001b[0m\u001b[36m \u001b[0m\u001b[2mnewspaper-creation-agent \u001b[0m                \u001b[2m│\u001b[0m\n",
              "\u001b[2m│\u001b[0m               \u001b[1m📦\u001b[0m\u001b[1m \u001b[0m\u001b[36mTransport:      \u001b[0m\u001b[36m \u001b[0m\u001b[2mStreamable-HTTP          \u001b[0m                \u001b[2m│\u001b[0m\n",
              "\u001b[2m│\u001b[0m               \u001b[1m🔗\u001b[0m\u001b[1m \u001b[0m\u001b[36mServer URL:     \u001b[0m\u001b[36m \u001b[0m\u001b[2mhttp://127.0.0.1:8080/mcp\u001b[0m                \u001b[2m│\u001b[0m\n",
              "\u001b[2m│\u001b[0m               \u001b[1m  \u001b[0m\u001b[1m \u001b[0m\u001b[36m                \u001b[0m\u001b[36m \u001b[0m\u001b[2m                         \u001b[0m                \u001b[2m│\u001b[0m\n",
              "\u001b[2m│\u001b[0m               \u001b[1m🏎️ \u001b[0m\u001b[1m \u001b[0m\u001b[36mFastMCP version:\u001b[0m\u001b[36m \u001b[0m\u001b[2;37m2.12.4                   \u001b[0m                \u001b[2m│\u001b[0m\n",
              "\u001b[2m│\u001b[0m               \u001b[1m🤝\u001b[0m\u001b[1m \u001b[0m\u001b[36mMCP SDK version:\u001b[0m\u001b[36m \u001b[0m\u001b[2;37m1.16.0                   \u001b[0m                \u001b[2m│\u001b[0m\n",
              "\u001b[2m│\u001b[0m               \u001b[1m  \u001b[0m\u001b[1m \u001b[0m\u001b[36m                \u001b[0m\u001b[36m \u001b[0m\u001b[2m                         \u001b[0m                \u001b[2m│\u001b[0m\n",
              "\u001b[2m│\u001b[0m               \u001b[1m📚\u001b[0m\u001b[1m \u001b[0m\u001b[36mDocs:           \u001b[0m\u001b[36m \u001b[0m\u001b[2mhttps://gofastmcp.com    \u001b[0m                \u001b[2m│\u001b[0m\n",
              "\u001b[2m│\u001b[0m               \u001b[1m🚀\u001b[0m\u001b[1m \u001b[0m\u001b[36mDeploy:         \u001b[0m\u001b[36m \u001b[0m\u001b[2mhttps://fastmcp.cloud    \u001b[0m                \u001b[2m│\u001b[0m\n",
              "\u001b[2m│\u001b[0m                                                                            \u001b[2m│\u001b[0m\n",
              "\u001b[2m╰────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
              "\n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[10/08/25 16:45:22] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Starting MCP server <span style=\"color: #008000; text-decoration-color: #008000\">'newspaper-creation-agent'</span> with transport           <a href=\"file:///Users/adi/Documents/GitHub/agentic-ai-workshop-2025/.venv/lib/python3.13/site-packages/fastmcp/server/server.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">server.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/adi/Documents/GitHub/agentic-ai-workshop-2025/.venv/lib/python3.13/site-packages/fastmcp/server/server.py#1579\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1579</span></a>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">'streamable-http'</span> on <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://127.0.0.1:8080/mcp</span>                          <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[2;36m[10/08/25 16:45:22]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Starting MCP server \u001b[32m'newspaper-creation-agent'\u001b[0m with transport           \u001b]8;id=173716;file:///Users/adi/Documents/GitHub/agentic-ai-workshop-2025/.venv/lib/python3.13/site-packages/fastmcp/server/server.py\u001b\\\u001b[2mserver.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=749615;file:///Users/adi/Documents/GitHub/agentic-ai-workshop-2025/.venv/lib/python3.13/site-packages/fastmcp/server/server.py#1579\u001b\\\u001b[2m1579\u001b[0m\u001b]8;;\u001b\\\n",
              "\u001b[2;36m                    \u001b[0m         \u001b[32m'streamable-http'\u001b[0m on \u001b[4;94mhttp://127.0.0.1:8080/mcp\u001b[0m                          \u001b[2m              \u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:     Started server process [14260]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://127.0.0.1:8080 (Press CTRL+C to quit)\n",
            "INFO:     Shutting down\n",
            "ERROR:    Cancel 0 running task(s), timeout graceful shutdown exceeded\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [14260]\n"
          ]
        }
      ],
      "source": [
        "os.system(\"lsof -ti:8080 | xargs kill -9 2>/dev/null\")\n",
        "await mcp.run_async(transport=\"streamable-http\", port=8080)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "380502f5",
      "metadata": {},
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "<img src=\"https://github.com/adityaarunsinghal/agentic-ai-workshop-2025/blob/main/notebooks/media/mcp_restaurant_13_llm_reasoning.png?raw=true\" width=\"500\" alt=\"mcp_restaurant_13_llm_reasoning\">\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b107b45",
      "metadata": {},
      "source": [
        "## If we had used the lower level MCP python SDK, our code would have been something like this:\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "Low-level MCP SDK translation of newspaper-agent-server\n",
        "\"\"\"\n",
        "import asyncio\n",
        "from typing import Any\n",
        "\n",
        "import mcp.server.stdio\n",
        "import mcp.types as types\n",
        "from mcp.server.lowlevel import NotificationOptions, Server\n",
        "from mcp.server.models import InitializationOptions\n",
        "\n",
        "# Create server instance\n",
        "server = Server(\"newspaper-agent-server-v1\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# PROMPT HANDLERS\n",
        "# ============================================================================\n",
        "\n",
        "@server.list_prompts()\n",
        "async def list_prompts() -> list[types.Prompt]:\n",
        "    \"\"\"List available prompts - replaces @mcp.prompt() decorator\"\"\"\n",
        "    return [\n",
        "        types.Prompt(\n",
        "            name=\"Detailed Newspaper Request\",\n",
        "            description=\"Prompt for quick newspaper request\",\n",
        "            arguments=[],  # No arguments for this prompt\n",
        "        )\n",
        "    ]\n",
        "\n",
        "\n",
        "@server.get_prompt()\n",
        "async def get_prompt(name: str, arguments: dict[str, str] | None) -> types.GetPromptResult:\n",
        "    \"\"\"Handle prompt requests - executes the prompt logic\"\"\"\n",
        "    if name == \"Detailed Newspaper Request\":\n",
        "        message_text = (\n",
        "            \"Please make me a detailed newspaper by pulling 30 HN articles, \"\n",
        "            \"ranking them all by how interesting they are and summarizing them \"\n",
        "            \"for me with broad themes at the top.\"\n",
        "        )\n",
        "        return types.GetPromptResult(\n",
        "            description=\"Prompt for detailed newspaper request\",\n",
        "            messages=[\n",
        "                types.PromptMessage(\n",
        "                    role=\"user\",\n",
        "                    content=types.TextContent(\n",
        "                        type=\"text\",\n",
        "                        text=message_text\n",
        "                    )\n",
        "                )\n",
        "            ]\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown prompt: {name}\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# TOOL HANDLERS\n",
        "# ============================================================================\n",
        "\n",
        "@server.list_tools()\n",
        "async def list_tools() -> list[types.Tool]:\n",
        "    \"\"\"List available tools - replaces @mcp.tool() decorator registration\"\"\"\n",
        "    return [\n",
        "        types.Tool(\n",
        "            name=\"fetch_hacker_news_stories_and_contents\",\n",
        "            description=(\n",
        "                \"Fetch top stories from Hacker News with titles, scores, and URLs. \"\n",
        "                \"Retrieves the most popular current stories from Hacker News, \"\n",
        "                \"formatted as a markdown list with essential metadata.\"\n",
        "            ),\n",
        "            inputSchema={\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"count\": {\n",
        "                        \"type\": \"integer\",\n",
        "                        \"description\": \"Number of stories to fetch (1-150)\",\n",
        "                        \"default\": 5,\n",
        "                        \"minimum\": 1,\n",
        "                        \"maximum\": 150\n",
        "                    }\n",
        "                },\n",
        "                \"required\": []  # count has a default, so it's optional\n",
        "            }\n",
        "        )\n",
        "    ]\n",
        "\n",
        "\n",
        "@server.call_tool()\n",
        "async def call_tool(name: str, arguments: dict[str, Any]) -> list[types.TextContent | types.ImageContent | types.EmbeddedResource]:\n",
        "    \"\"\"Handle tool calls - replaces @mcp.tool() decorated function execution\"\"\"\n",
        "    \n",
        "    if name == \"fetch_hacker_news_stories_and_contents\":\n",
        "        # Extract arguments with defaults\n",
        "        count = arguments.get(\"count\", 5)\n",
        "        \n",
        "        # Validate input parameters\n",
        "        if not 1 <= count <= 150:\n",
        "            raise ValueError(\"Count must be between 1 and 150\")\n",
        "\n",
        "        client = HackerNewsClient()\n",
        "\n",
        "        # Fetch story IDs from top stories endpoint\n",
        "        story_ids = await client.get_story_ids(\"topstories\", count)\n",
        "\n",
        "        # Fetch detailed story information\n",
        "        stories = []\n",
        "        for story_id in story_ids:\n",
        "            story = await client.get_item(story_id)\n",
        "            if story and story.get(\"title\"):\n",
        "                url = story.get(\"url\", f\"{HN_WEBSITE_BASE}/item?id={story_id}\")\n",
        "                content = await fetch(url)\n",
        "                stories.append({\n",
        "                    \"title\": story[\"title\"],\n",
        "                    \"score\": story.get(\"score\", 0),\n",
        "                    \"url\": url,\n",
        "                    \"content\": content,\n",
        "                })\n",
        "\n",
        "        # Format as markdown with numbered list\n",
        "        result = f\"# Top {len(stories)} Hacker News Stories\\n\\n\"\n",
        "        for i, story in enumerate(stories, 1):\n",
        "            result += f\"{i}. **{story['title']}** ({story['score']} points)\\n\"\n",
        "            result += f\"   {story['url']}\\n\\n\"\n",
        "            result += f\"   {story['content']}\\n\\n\"\n",
        "\n",
        "        # Return as TextContent (low-level requires specific content types)\n",
        "        return [types.TextContent(type=\"text\", text=result)]\n",
        "    \n",
        "    else:\n",
        "        raise ValueError(f\"Unknown tool: {name}\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# SERVER RUNNER\n",
        "# ============================================================================\n",
        "\n",
        "async def run():\n",
        "    \"\"\"Run the MCP server with stdio transport\"\"\"\n",
        "    async with mcp.server.stdio.stdio_server() as (read_stream, write_stream):\n",
        "        await server.run(\n",
        "            read_stream,\n",
        "            write_stream,\n",
        "            InitializationOptions(\n",
        "                server_name=\"newspaper-agent-server-v1\",\n",
        "                server_version=\"1.0.0\",\n",
        "                capabilities=server.get_capabilities(\n",
        "                    notification_options=NotificationOptions(),\n",
        "                    experimental_capabilities={},\n",
        "                ),\n",
        "            ),\n",
        "        )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(run())\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "homework",
      "metadata": {},
      "source": [
        "---\n",
        "# Cliffhanger for Session 2:\n",
        "\n",
        "- Semantic compression\n",
        "- Memory systems (short vs long-term)\n",
        "- Context management strategies\n",
        "- Smart summarization\n",
        "- Tool result caching\n",
        "\n",
        "---\n",
        "\n",
        "# Homework & Next Steps\n",
        "\n",
        "## Required (for participation):\n",
        "1. **Think of one agentic use case** relevant to your work/research\n",
        "2. **Write it down** - we'll discuss next session\n",
        "\n",
        "## Encouraged (for learning):\n",
        "1. **Run this notebook** end-to-end\n",
        "2. **Implement 2 new tools** for your use case\n",
        "3. **Document what worked** and what didn't\n",
        "4. **Try connecting tools** together\n",
        "\n",
        "## For the Ambitious:\n",
        "- Some of you worried this wouldn't be technical enough... \n",
        "- Build a complex tool (database integration, API calls)\n",
        "- Create tool compositions\n",
        "- Implement your own mini-agent\n",
        "\n",
        "## Resources:\n",
        "- This notebook: Available immediately\n",
        "- Full server code: In the repository\n",
        "- Office hours: Friday 5pm ET\n",
        "\n",
        "## Session 2 Preview:\n",
        "- **Context Management**: How to handle infinite content\n",
        "- **Memory Systems**: Short-term vs long-term\n",
        "- **Elicitation**: Getting info from users\n",
        "- **Sampling**: Using LLMs within tools\n",
        "- **Production Patterns**: What works in the real world\n",
        "\n",
        "## Session 4 Reminder:\n",
        "**Showcase Day!** We'll all share what we built and learn from each other.\n",
        "\n",
        "---\n",
        "\n",
        "# Thank You! 🎉\n",
        "\n",
        "\n",
        "**See you next week for Session 2!**\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
