{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e4a7f4a3",
      "metadata": {},
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "# NYU Agentic AI Workshop - Session 2\n",
        "\n",
        "## Advanced MCP Features and Agentic AI Survey\n",
        "\n",
        "### Part 2: Making \"smart\" tools to help clients preserve context\n",
        "\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aeb830c9",
      "metadata": {},
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "<img src=\"https://github.com/adityaarunsinghal/agentic-ai-workshop-2025/blob/main/notebooks/media/mcp_restaurant_11_server_isolation.png?raw=true\" width=\"500\" alt=\"mcp_restaurant_11_server_isolation\">\n",
        "<!-- <img src=\"https://github.com/adityaarunsinghal/agentic-ai-workshop-2025/blob/main/notebooks/media/mcp_restaurant_11_server_isolation.png?raw=true\" width=\"500\" alt=\"mcp_restaurant_11_server_isolation\"> -->\n",
        "\n",
        "### The different kitchens never learn each others' secrets\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/adityaarunsinghal/agentic-ai-workshop-2025/blob/main/notebooks/media/mcp_restaurant_14_json_rpc.png?raw=true\" width=\"500\" alt=\"mcp_restaurant_14_json_rpc\">\n",
        "<!-- <img src=\"https://github.com/adityaarunsinghal/agentic-ai-workshop-2025/blob/main/notebooks/media/mcp_restaurant_14_json_rpc.png?raw=true\" width=\"500\" alt=\"mcp_restaurant_14_json_rpc\"> -->\n",
        "\n",
        "### All conversations follow a standard format\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b145b799",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ All imports successful!\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "import os\n",
        "import sys\n",
        "from contextlib import asynccontextmanager\n",
        "from dataclasses import dataclass\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from typing import Dict, List\n",
        "\n",
        "import nest_asyncio\n",
        "from dotenv import load_dotenv\n",
        "from fastmcp import Context, FastMCP\n",
        "\n",
        "# Load environment\n",
        "env_path = Path.cwd().parent / \".env\"\n",
        "load_dotenv(env_path)\n",
        "\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "# Add server to path\n",
        "sys.path.insert(0, str(Path.cwd().parent / \"src\" / \"server\"))\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Service imports\n",
        "from src.server.config.settings import get_settings\n",
        "from src.server.services.article_memory_v2 import ArticleMemoryService\n",
        "from src.server.services.email_service import EmailService\n",
        "from src.server.services.http_client import HackerNewsClient, fetch_content\n",
        "from src.server.services.interests_file import InterestsFileService\n",
        "from src.server.services.newspaper_service import NewspaperService\n",
        "\n",
        "print(\"‚úÖ All imports successful!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5fe81803",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# APPLICATION CONTEXT\n",
        "# ============================================================================\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class AppContext:\n",
        "    \"\"\"Application context with all services.\"\"\"\n",
        "\n",
        "    hn_client: HackerNewsClient\n",
        "    interests_service: InterestsFileService\n",
        "    article_memory: ArticleMemoryService\n",
        "    newspaper_service: NewspaperService\n",
        "    email_service: EmailService\n",
        "    settings: object\n",
        "\n",
        "\n",
        "@asynccontextmanager\n",
        "async def app_lifespan(mcp: FastMCP):\n",
        "    \"\"\"Initialize all services for the newspaper agent.\"\"\"\n",
        "    print(\"üöÄ Starting Advanced Newspaper Agent MCP Server\")\n",
        "\n",
        "    settings = get_settings()\n",
        "    settings.data_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Initialize services\n",
        "    hn_client = HackerNewsClient()\n",
        "    interests_service = InterestsFileService(settings.data_dir)\n",
        "    article_memory = ArticleMemoryService()\n",
        "    article_memory.initialize(settings.data_dir / \"chromadb\")\n",
        "    newspaper_service = NewspaperService(settings.data_dir)\n",
        "\n",
        "    # Email service\n",
        "    email_service = EmailService(\n",
        "        {\n",
        "            \"server\": \"smtp.gmail.com\",\n",
        "            \"port\": 465,\n",
        "            \"use_tls\": False,\n",
        "            \"use_ssl\": True,\n",
        "            \"username\": os.getenv(\"MCP_SMTP_FROM_EMAIL\", \"\"),\n",
        "            \"password\": os.getenv(\"MCP_SMTP_PASSWORD\", \"\"),\n",
        "            \"from_email\": os.getenv(\"MCP_SMTP_FROM_EMAIL\", \"\"),\n",
        "            \"from_name\": \"AI Newspaper Agent\",\n",
        "        }\n",
        "    )\n",
        "\n",
        "    print(\"‚úÖ All services initialized!\")\n",
        "\n",
        "    try:\n",
        "        yield AppContext(\n",
        "            hn_client=hn_client,\n",
        "            interests_service=interests_service,\n",
        "            article_memory=article_memory,\n",
        "            newspaper_service=newspaper_service,\n",
        "            email_service=email_service,\n",
        "            settings=settings,\n",
        "        )\n",
        "    finally:\n",
        "        print(\"üëã Shutting down MCP Server\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d36c5892",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# MCP SERVER\n",
        "# ============================================================================\n",
        "\n",
        "mcp = FastMCP(\n",
        "    name=\"advanced-newspaper-agent\",\n",
        "    instructions=\"\"\"Advanced newspaper creation system with intelligent content discovery and composition.\n",
        "\n",
        "AGENT ROLE: Strategic Editor\n",
        "- Decide WHICH stories to cover\n",
        "- Choose editorial ANGLE and DEPTH\n",
        "- Control STRUCTURE and POLISH\n",
        "- Delegate tactical execution to smart tools\n",
        "\n",
        "SMART FEATURES:\n",
        "- Content discovery automatically stores in ChromaDB with clean content IDs\n",
        "- Context (interests, past coverage) flows automatically via sampling\n",
        "- One tool call adds multiple articles with full formatting\n",
        "- Elicitation enables interactive workflows\n",
        "- Quality validation enforces standards\n",
        "\n",
        "CORE WORKFLOW:\n",
        "1. discover_stories() ‚Üí Get enriched story list with content IDs\n",
        "2. quick_look() ‚Üí Preview any content by ID\n",
        "3. create_newspaper() ‚Üí Start with smart defaults\n",
        "4. add_content_cluster() ‚Üí Add multiple articles in one call\n",
        "5. Polish with section/article controls\n",
        "6. validate_and_finalize() ‚Üí Enforce quality standards\n",
        "7. publish_newspaper() ‚Üí Deliver\n",
        "\n",
        "Current date: {datetime.now().strftime('%A, %B %d, %Y')}\"\"\",\n",
        "    lifespan=app_lifespan,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "529eec34",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# RESOURCES\n",
        "# ============================================================================\n",
        "\n",
        "\n",
        "@mcp.resource(\"file://interests.md\")\n",
        "async def get_interests_resource(ctx: Context = None) -> str:\n",
        "    \"\"\"User's interests - topics, sources, preferences.\n",
        "\n",
        "    Annotations: High priority, for assistant consumption.\"\"\"\n",
        "    interests_service = ctx.request_context.lifespan_context.interests_service\n",
        "    interests = interests_service.read_interests()\n",
        "\n",
        "    content = \"# User Interests\\n\\n\"\n",
        "    content += f\"**Last Updated:** {interests.get('last_updated', 'Unknown')}\\n\\n\"\n",
        "\n",
        "    if interests.get(\"topics\"):\n",
        "        content += \"## Topics\\n\"\n",
        "        for topic in interests[\"topics\"]:\n",
        "            content += f\"- {topic}\\n\"\n",
        "        content += \"\\n\"\n",
        "\n",
        "    if interests.get(\"sources\"):\n",
        "        content += \"## Preferred Sources\\n\"\n",
        "        for source in interests[\"sources\"]:\n",
        "            content += f\"- {source}\\n\"\n",
        "        content += \"\\n\"\n",
        "\n",
        "    content += f\"## Summary Style\\n- {interests.get('style', 'detailed')}\\n\\n\"\n",
        "\n",
        "    if interests.get(\"notes\"):\n",
        "        content += \"## Notes\\n\"\n",
        "        for note in interests[\"notes\"]:\n",
        "            content += f\"- {note}\\n\"\n",
        "\n",
        "    return content\n",
        "\n",
        "\n",
        "@mcp.resource(\"memory://context-summary\")\n",
        "async def get_context_summary(ctx: Context = None) -> str:\n",
        "    \"\"\"Live summary of archive - recent newspapers, trending topics, coverage gaps.\n",
        "\n",
        "    Annotations: High priority, refreshes every 5 minutes.\"\"\"\n",
        "    article_memory = ctx.request_context.lifespan_context.article_memory\n",
        "    interests_service = ctx.request_context.lifespan_context.interests_service\n",
        "\n",
        "    # Get stats\n",
        "    stats = article_memory.get_context_summary()\n",
        "    interests = interests_service.read_interests()\n",
        "    user_topics = set(interests.get(\"topics\", []))\n",
        "\n",
        "    content = \"# Archive Context Summary\\n\\n\"\n",
        "\n",
        "    content += \"## Recent Activity\\n\"\n",
        "    content += f\"- **Total Articles Stored:** {stats['total_articles']}\\n\"\n",
        "    content += f\"- **Newspapers Created:** {stats['total_newspapers']}\\n\"\n",
        "    content += f\"- **Last Article Added:** {stats['last_article_date']}\\n\\n\"\n",
        "\n",
        "    if stats[\"recent_newspapers\"]:\n",
        "        content += \"## Recent Newspapers (Last 5)\\n\"\n",
        "        for paper in stats[\"recent_newspapers\"]:\n",
        "            content += f\"- **{paper['title']}** ({paper['date']}) - {paper['reading_time']}min, {paper['article_count']} articles\\n\"\n",
        "        content += \"\\n\"\n",
        "\n",
        "    if stats[\"trending_topics\"]:\n",
        "        content += \"## Trending Topics (Most Covered)\\n\"\n",
        "        for topic, count in stats[\"trending_topics\"]:\n",
        "            content += f\"- {topic}: {count} articles\\n\"\n",
        "        content += \"\\n\"\n",
        "\n",
        "    # Calculate coverage gaps\n",
        "    covered_topics = {topic for topic, _ in stats[\"trending_topics\"]}\n",
        "    gaps = user_topics - covered_topics\n",
        "\n",
        "    if gaps:\n",
        "        content += \"## Coverage Gaps (Interests Not Recently Covered)\\n\"\n",
        "        for gap in gaps:\n",
        "            content += f\"- {gap}\\n\"\n",
        "        content += \"\\n\"\n",
        "\n",
        "    return content\n",
        "\n",
        "\n",
        "@mcp.resource(\"memory://articles/{topic}\")\n",
        "async def get_articles_by_topic(topic: str, ctx: Context = None) -> str:\n",
        "    \"\"\"Dynamic resource template - search articles by topic.\n",
        "\n",
        "    Example: memory://articles/distributed-systems\n",
        "    Returns semantic search results for that topic.\"\"\"\n",
        "    article_memory = ctx.request_context.lifespan_context.article_memory\n",
        "\n",
        "    articles = article_memory.search_articles(query=topic, limit=10)\n",
        "\n",
        "    if not articles:\n",
        "        return f\"# No articles found for '{topic}'\\n\\nTry broader search terms or different topics.\"\n",
        "\n",
        "    content = f\"# Articles about '{topic}' ({len(articles)} found)\\n\\n\"\n",
        "\n",
        "    for i, article in enumerate(articles, 1):\n",
        "        content += f\"## {i}. {article['title']}\\n\"\n",
        "        content += f\"**Content ID:** {article.get('content_id', 'unknown')}\\n\"\n",
        "        content += f\"**Similarity:** {article['similarity']:.1%} | \"\n",
        "        content += f\"**Source:** {article['source']} | \"\n",
        "        content += f\"**Reading Time:** {article.get('reading_time', '?')} min\\n\"\n",
        "        if article[\"topics\"]:\n",
        "            content += f\"**Topics:** {', '.join(article['topics'])}\\n\"\n",
        "        content += f\"**URL:** {article['url']}\\n\\n\"\n",
        "        if article.get(\"summary\"):\n",
        "            content += f\"{article['summary']}\\n\\n\"\n",
        "        content += \"---\\n\\n\"\n",
        "\n",
        "    return content\n",
        "\n",
        "\n",
        "@mcp.resource(\"memory://newspapers/recent\")\n",
        "async def get_recent_newspapers(ctx: Context = None) -> str:\n",
        "    \"\"\"Recent newspapers for structural reference and avoiding repetition.\n",
        "\n",
        "    Shows last 5 newspapers with structure, topics, and metadata.\"\"\"\n",
        "    article_memory = ctx.request_context.lifespan_context.article_memory\n",
        "\n",
        "    newspapers = article_memory.search_newspapers(days_back=30)[:5]\n",
        "\n",
        "    if not newspapers:\n",
        "        return \"# No Recent Newspapers\\n\\nThis is your first newspaper!\"\n",
        "\n",
        "    content = f\"# Recent Newspapers ({len(newspapers)})\\n\\n\"\n",
        "\n",
        "    for paper in newspapers:\n",
        "        content += f\"## {paper['title']}\\n\"\n",
        "        content += f\"**ID:** {paper['newspaper_id']}\\n\"\n",
        "        content += f\"**Date:** {paper['timestamp'][:10]}\\n\"\n",
        "        content += f\"**Type:** {paper['edition_type']} | \"\n",
        "        content += f\"**Articles:** {paper['article_count']} | \"\n",
        "        content += f\"**Reading Time:** {paper['reading_time']} min\\n\"\n",
        "        if paper[\"topics\"]:\n",
        "            content += f\"**Topics:** {', '.join(paper['topics'])}\\n\"\n",
        "        if paper.get(\"tone\"):\n",
        "            content += f\"**Tone:** {paper['tone']}\\n\"\n",
        "\n",
        "        # Show structure if available\n",
        "        if paper.get(\"structure\"):\n",
        "            content += (\n",
        "                f\"**Structure:** {', '.join(paper['structure'].get('sections', []))}\\n\"\n",
        "            )\n",
        "\n",
        "        content += \"\\n---\\n\\n\"\n",
        "\n",
        "    return content"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98aadae7",
      "metadata": {},
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "<img src=\"https://github.com/adityaarunsinghal/agentic-ai-workshop-2025/blob/main/notebooks/media/mcp_restaurant_09_notifications.png?raw=true\" width=\"500\" alt=\"mcp_restaurant_09_notifications\">\n",
        "<!-- <img src=\"https://github.com/adityaarunsinghal/agentic-ai-workshop-2025/blob/main/notebooks/media/mcp_restaurant_09_notifications.png?raw=true\" width=\"500\" alt=\"mcp_restaurant_09_notifications\"> -->\n",
        "\n",
        "### The servers let the clients know when something changes\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/adityaarunsinghal/agentic-ai-workshop-2025/blob/main/notebooks/media/mcp_restaurant_10_progress.png?raw=true\" width=\"500\" alt=\"mcp_restaurant_10_progress\">\n",
        "<!-- <img src=\"https://github.com/adityaarunsinghal/agentic-ai-workshop-2025/blob/main/notebooks/media/mcp_restaurant_10_progress.png?raw=true\" width=\"500\" alt=\"mcp_restaurant_10_progress\"> -->\n",
        "\n",
        "### They (kitchens) can also keep Ingrid and the waiters (MCP clients) in the know about where the dishes are in their prep!!\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "834352fe",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# TOOLS: CONTENT DISCOVERY\n",
        "# ============================================================================\n",
        "\n",
        "\n",
        "@mcp.tool()\n",
        "async def discover_stories(\n",
        "    query: str = \"tech news\",\n",
        "    count: int = 20,\n",
        "    sources: List[str] = None,\n",
        "    ctx: Context = None,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Discover stories from multiple sources and store with content IDs.\n",
        "\n",
        "    This is the PRIMARY discovery tool. It:\n",
        "    1. Fetches stories from sources (HN, web, etc.)\n",
        "    2. Stores full content in ChromaDB automatically\n",
        "    3. Generates clean content IDs for easy reference\n",
        "    4. Scores relevance against user interests\n",
        "    5. Finds related past coverage\n",
        "    6. Returns enriched summaries for decision-making\n",
        "\n",
        "    Args:\n",
        "        query: Search query (e.g., \"AI ethics\", \"distributed systems\")\n",
        "        count: Number of stories to fetch (1-30)\n",
        "        sources: List of sources [\"hn\", \"web\", \"perplexity\"] (default: [\"hn\"])\n",
        "\n",
        "    Returns:\n",
        "        Formatted list of stories with content IDs, titles, relevance scores,\n",
        "        and metadata. Use content IDs with other tools like quick_look() or\n",
        "        add_content_cluster().\n",
        "\n",
        "    Example:\n",
        "        discover_stories(\"AI ethics\", count=20, sources=[\"hn\", \"web\"])\n",
        "        ‚Üí Returns stories with IDs like cnt_hn_20251005_abc123\n",
        "    \"\"\"\n",
        "    if not 1 <= count <= 30:\n",
        "        return \"‚ùå Count must be between 1 and 30\"\n",
        "\n",
        "    sources = sources or [\"hn\"]\n",
        "\n",
        "    hn_client = ctx.request_context.lifespan_context.hn_client\n",
        "    article_memory = ctx.request_context.lifespan_context.article_memory\n",
        "    interests_service = ctx.request_context.lifespan_context.interests_service\n",
        "\n",
        "    await ctx.info(f\"üîç Discovering {count} stories from {sources}...\")\n",
        "\n",
        "    # Read user interests\n",
        "    interests = interests_service.read_interests()\n",
        "    user_topics = interests.get(\"topics\", [])\n",
        "\n",
        "    enriched_stories = []\n",
        "\n",
        "    # Fetch from Hacker News\n",
        "    if \"hn\" in sources:\n",
        "        await ctx.report_progress(progress=0, total=count)\n",
        "\n",
        "        story_ids = await hn_client.get_story_ids(\"topstories\", count)\n",
        "\n",
        "        for i, story_id in enumerate(story_ids):\n",
        "            story = await hn_client.get_item(story_id)\n",
        "            if not story or not story.get(\"title\"):\n",
        "                continue\n",
        "\n",
        "            await ctx.report_progress(progress=i + 1, total=count)\n",
        "\n",
        "            # Generate content ID\n",
        "            content_id = f\"cnt_hn_{datetime.now().strftime('%Y%m%d')}_{abs(hash(story['url'])) % 10000:04d}\"\n",
        "\n",
        "            # Fetch full content\n",
        "            try:\n",
        "                full_content = await fetch_content(story[\"url\"], max_length=8000)\n",
        "            except:\n",
        "                full_content = story.get(\"title\", \"\")\n",
        "\n",
        "            # Calculate relevance (simple keyword matching)\n",
        "            title_lower = story[\"title\"].lower()\n",
        "            relevance = sum(1 for topic in user_topics if topic.lower() in title_lower)\n",
        "            relevance_score = min(1.0, relevance / max(1, len(user_topics)))\n",
        "\n",
        "            # Extract matching topics\n",
        "            topics = [topic for topic in user_topics if topic.lower() in title_lower]\n",
        "\n",
        "            # Store in ChromaDB\n",
        "            await ctx.debug(f\"Storing content_id: {content_id}\")\n",
        "            article_memory.store_article_with_content_id(\n",
        "                content_id=content_id,\n",
        "                url=story[\"url\"],\n",
        "                content=full_content,\n",
        "                title=story[\"title\"],\n",
        "                source=\"hn\",\n",
        "                topics=topics,\n",
        "                summary=\"\",\n",
        "            )\n",
        "\n",
        "            # Search for related past articles\n",
        "            related = article_memory.search_articles(query=story[\"title\"], limit=3)\n",
        "\n",
        "            enriched_stories.append(\n",
        "                {\n",
        "                    \"content_id\": content_id,\n",
        "                    \"title\": story[\"title\"],\n",
        "                    \"url\": story[\"url\"],\n",
        "                    \"source\": \"hn\",\n",
        "                    \"score\": story.get(\"score\", 0),\n",
        "                    \"relevance_score\": relevance_score,\n",
        "                    \"word_count\": len(full_content.split()),\n",
        "                    \"estimated_reading_time\": max(1, len(full_content.split()) // 200),\n",
        "                    \"related_past_articles\": [a[\"title\"] for a in related],\n",
        "                    \"topics\": topics,\n",
        "                }\n",
        "            )\n",
        "\n",
        "    # Sort by relevance\n",
        "    enriched_stories.sort(key=lambda x: x[\"relevance_score\"], reverse=True)\n",
        "\n",
        "    # Format output\n",
        "    result = f\"# üì∞ Discovered {len(enriched_stories)} Stories\\n\\n\"\n",
        "    result += f\"**Query:** {query}\\n\"\n",
        "    result += f\"**Sources:** {', '.join(sources)}\\n\"\n",
        "    result += \"**Sorted by:** Relevance to your interests\\n\\n\"\n",
        "\n",
        "    for i, story in enumerate(enriched_stories, 1):\n",
        "        result += f\"## {i}. {story['title']}\\n\"\n",
        "        result += f\"**Content ID:** `{story['content_id']}`\\n\"\n",
        "        result += f\"**Relevance:** {story['relevance_score']:.0%} | \"\n",
        "        result += f\"**Reading Time:** {story['estimated_reading_time']}min | \"\n",
        "        result += f\"**HN Score:** {story['score']}\\n\"\n",
        "        if story[\"topics\"]:\n",
        "            result += f\"**Matching Interests:** {', '.join(story['topics'])}\\n\"\n",
        "        if story[\"related_past_articles\"]:\n",
        "            result += f\"**Related Past Coverage:** {len(story['related_past_articles'])} articles\\n\"\n",
        "        result += f\"**URL:** {story['url']}\\n\\n\"\n",
        "\n",
        "    await ctx.info(f\"‚úÖ Discovered and stored {len(enriched_stories)} stories\")\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "@mcp.tool()\n",
        "async def quick_look(content_ids: List[str], ctx: Context = None) -> str:\n",
        "    \"\"\"\n",
        "    Quick preview of stored content by content ID.\n",
        "\n",
        "    Use this to review content before adding to newspaper.\n",
        "    Shows title, metadata, and first 300 characters.\n",
        "\n",
        "    Args:\n",
        "        content_ids: List of content IDs to preview\n",
        "\n",
        "    Returns:\n",
        "        Compact preview of each content item\n",
        "\n",
        "    Example:\n",
        "        quick_look([\"cnt_hn_20251005_1234\", \"cnt_hn_20251005_5678\"])\n",
        "    \"\"\"\n",
        "    article_memory = ctx.request_context.lifespan_context.article_memory\n",
        "\n",
        "    previews = []\n",
        "    for content_id in content_ids:\n",
        "        article = article_memory.get_by_content_id(content_id)\n",
        "\n",
        "        if article:\n",
        "            previews.append(article)\n",
        "        else:\n",
        "            previews.append({\"content_id\": content_id, \"error\": \"Not found\"})\n",
        "\n",
        "    # Format output\n",
        "    result = f\"# üëÄ Quick Look: {len(content_ids)} items\\n\\n\"\n",
        "\n",
        "    for preview in previews:\n",
        "        if \"error\" in preview:\n",
        "            result += f\"## ‚ùå {preview['content_id']}\\n\"\n",
        "            result += f\"**Error:** {preview['error']}\\n\\n\"\n",
        "            continue\n",
        "\n",
        "        result += f\"## {preview['title']}\\n\"\n",
        "        result += f\"**Content ID:** `{preview['content_id']}`\\n\"\n",
        "        result += f\"**Word Count:** {preview['word_count']} | \"\n",
        "        result += f\"**Reading Time:** {preview.get('reading_time', '?')}min\\n\"\n",
        "        if preview.get(\"topics\"):\n",
        "            result += f\"**Topics:** {', '.join(preview['topics'])}\\n\"\n",
        "        result += f\"\\n**Preview:**\\n{preview['content_preview']}\\n\\n\"\n",
        "        result += \"---\\n\\n\"\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "06174ada",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# TOOLS: NEWSPAPER CREATION\n",
        "# ============================================================================\n",
        "\n",
        "\n",
        "@mcp.tool()\n",
        "async def create_newspaper(\n",
        "    title: str,\n",
        "    type: str = \"deep_dive\",\n",
        "    subtitle: str = \"\",\n",
        "    structure_template: str = None,\n",
        "    ctx: Context = None,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Create new newspaper with smart defaults.\n",
        "\n",
        "    Sets up newspaper structure based on type with pre-configured sections\n",
        "    and target reading times.\n",
        "\n",
        "    Args:\n",
        "        title: Newspaper title\n",
        "        type: Edition type - determines structure and targets\n",
        "              - \"morning_brief\": 2 sections, 15min target\n",
        "              - \"deep_dive\": 3-4 thematic sections, 35min target\n",
        "              - \"thematic\": Custom sections, agent-driven\n",
        "              - \"follow_up\": Timeline + Analysis, 20min target\n",
        "        subtitle: Optional subtitle\n",
        "        structure_template: Optional newspaper_id to copy structure from\n",
        "\n",
        "    Returns:\n",
        "        Newspaper ID and configuration details\n",
        "    \"\"\"\n",
        "    newspaper_service = ctx.request_context.lifespan_context.newspaper_service\n",
        "\n",
        "    # Type-specific defaults\n",
        "    type_configs = {\n",
        "        \"morning_brief\": {\n",
        "            \"sections\": [\"Breaking News\", \"Quick Reads\"],\n",
        "            \"target_reading_time\": 15,\n",
        "            \"suggested_articles\": 7,\n",
        "        },\n",
        "        \"deep_dive\": {\n",
        "            \"sections\": [\"Topic 1\", \"Topic 2\", \"Topic 3\"],\n",
        "            \"target_reading_time\": 35,\n",
        "            \"suggested_articles\": 10,\n",
        "        },\n",
        "        \"thematic\": {\n",
        "            \"sections\": [],  # Agent will define\n",
        "            \"target_reading_time\": 25,\n",
        "            \"suggested_articles\": 8,\n",
        "        },\n",
        "        \"follow_up\": {\n",
        "            \"sections\": [\"What Changed\", \"Deep Analysis\"],\n",
        "            \"target_reading_time\": 20,\n",
        "            \"suggested_articles\": 6,\n",
        "        },\n",
        "    }\n",
        "\n",
        "    config = type_configs.get(type, type_configs[\"deep_dive\"])\n",
        "\n",
        "    # Create newspaper\n",
        "    result = newspaper_service.create_draft(title, subtitle, type)\n",
        "\n",
        "    if not result[\"success\"]:\n",
        "        return f\"‚ùå {result.get('error', 'Failed to create newspaper')}\"\n",
        "\n",
        "    newspaper_id = result[\"newspaper_id\"]\n",
        "\n",
        "    # Add pre-configured sections\n",
        "    for section_title in config[\"sections\"]:\n",
        "        newspaper_service.add_section(\n",
        "            newspaper_id,\n",
        "            section_title,\n",
        "            layout=\"featured\" if section_title == config[\"sections\"][0] else \"grid\",\n",
        "        )\n",
        "\n",
        "    # Set metadata\n",
        "    newspaper_service.set_metadata(\n",
        "        newspaper_id,\n",
        "        {\n",
        "            \"target_reading_time\": config[\"target_reading_time\"],\n",
        "            \"suggested_articles\": config[\"suggested_articles\"],\n",
        "        },\n",
        "    )\n",
        "\n",
        "    await ctx.info(f\"‚úÖ Created {type} newspaper: {newspaper_id}\")\n",
        "\n",
        "    response = \"# ‚úÖ Created Newspaper\\n\\n\"\n",
        "    response += f\"**ID:** `{newspaper_id}`\\n\"\n",
        "    response += f\"**Title:** {title}\\n\"\n",
        "    response += f\"**Type:** {type}\\n\"\n",
        "    response += f\"**Target Reading Time:** {config['target_reading_time']} minutes\\n\"\n",
        "    response += f\"**Suggested Articles:** {config['suggested_articles']}\\n\\n\"\n",
        "\n",
        "    if config[\"sections\"]:\n",
        "        response += \"**Pre-configured Sections:**\\n\"\n",
        "        for section in config[\"sections\"]:\n",
        "            response += f\"- {section}\\n\"\n",
        "\n",
        "    response += \"\\n**Next Steps:**\\n\"\n",
        "    response += \"1. Use add_content_cluster() to add articles\\n\"\n",
        "    response += \"2. Polish with section/article controls\\n\"\n",
        "    response += \"3. validate_and_finalize() to check quality\\n\"\n",
        "    response += \"4. publish_newspaper() to deliver\\n\"\n",
        "\n",
        "    return response\n",
        "\n",
        "\n",
        "@mcp.tool()\n",
        "async def add_content_cluster(\n",
        "    newspaper_id: str,\n",
        "    section: str,\n",
        "    content_ids: List[str],\n",
        "    treatment: str = \"detailed\",\n",
        "    auto_enhance: bool = True,\n",
        "    link_related: bool = True,\n",
        "    ctx: Context = None,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Add multiple related articles as a coherent cluster - THE WORKHORSE TOOL.\n",
        "\n",
        "    This tool does EVERYTHING in one call:\n",
        "    - Fetches content from ChromaDB (already stored by discover_stories)\n",
        "    - Reads interests automatically\n",
        "    - Finds related past coverage\n",
        "    - Uses sampling to generate summaries with full context\n",
        "    - Formats with rich elements (quotes, key points)\n",
        "    - Links related articles\n",
        "    - Updates newspaper\n",
        "\n",
        "    If content_ids > 5, may elicit user preference on depth.\n",
        "\n",
        "    Args:\n",
        "        newspaper_id: Target newspaper\n",
        "        section: Section to add to\n",
        "        content_ids: List of content IDs to add\n",
        "        treatment: Summary style\n",
        "                   - \"brief\": Quick overview (2-3 sentences)\n",
        "                   - \"detailed\": Comprehensive summary (6-8 sentences)\n",
        "                   - \"technical\": Focus on technical details\n",
        "        auto_enhance: Automatically add pull quotes and key points\n",
        "        link_related: Cross-reference articles in cluster\n",
        "\n",
        "    Returns:\n",
        "        Summary of articles added and enhancements applied\n",
        "    \"\"\"\n",
        "    if len(content_ids) == 0:\n",
        "        return \"‚ùå No content IDs provided\"\n",
        "\n",
        "    article_memory = ctx.request_context.lifespan_context.article_memory\n",
        "    newspaper_service = ctx.request_context.lifespan_context.newspaper_service\n",
        "    interests_service = ctx.request_context.lifespan_context.interests_service\n",
        "\n",
        "    await ctx.info(f\"üìù Adding {len(content_ids)} articles to '{section}'...\")\n",
        "\n",
        "    # Elicit if too many articles\n",
        "    if len(content_ids) > 5:\n",
        "        from dataclasses import make_dataclass\n",
        "\n",
        "        choice_response = await ctx.elicit(\n",
        "            message=f\"Found {len(content_ids)} articles. Include all (comprehensive) or top 5 (focused)?\",\n",
        "            response_type=make_dataclass(\"SelectionChoice\", [(\"selection\", str)]),\n",
        "        )\n",
        "\n",
        "        if (\n",
        "            choice_response.action == \"accept\"\n",
        "            and choice_response.data.selection == \"focused\"\n",
        "        ):\n",
        "            content_ids = content_ids[:5]\n",
        "            await ctx.info(\"üìä User selected focused approach - using top 5 articles\")\n",
        "\n",
        "    # Read interests\n",
        "    interests = interests_service.read_interests()\n",
        "\n",
        "    # Fetch all articles from ChromaDB\n",
        "    articles = []\n",
        "    for content_id in content_ids:\n",
        "        article = article_memory.get_by_content_id(content_id)\n",
        "        if article:\n",
        "            articles.append(article)\n",
        "        else:\n",
        "            await ctx.warning(f\"Content ID not found: {content_id}\")\n",
        "\n",
        "    if not articles:\n",
        "        return \"‚ùå No valid articles found\"\n",
        "\n",
        "    await ctx.report_progress(progress=0, total=len(articles))\n",
        "\n",
        "    # Process each article\n",
        "    added_articles = []\n",
        "    total_reading_time = 0\n",
        "\n",
        "    for i, article in enumerate(articles):\n",
        "        await ctx.debug(f\"Processing: {article['title']}\")\n",
        "\n",
        "        # Find related past coverage\n",
        "        related = article_memory.search_articles(query=article[\"content\"], limit=5)\n",
        "\n",
        "        # Use sampling to generate summary\n",
        "        await ctx.info(f\"ü§ñ Generating {treatment} summary with LLM...\")\n",
        "\n",
        "        sample_result = await ctx.sample(\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": {\n",
        "                        \"type\": \"text\",\n",
        "                        \"text\": f\"\"\"Summarize this article in {treatment} style.\n",
        "\n",
        "USER INTERESTS:\n",
        "{chr(10).join(f\"- {topic}\" for topic in interests.get(\"topics\", []))}\n",
        "\n",
        "PAST COVERAGE (for context):\n",
        "{chr(10).join(f\"- {r['title']}\" for r in related[:3])}\n",
        "\n",
        "ARTICLE TO SUMMARIZE:\n",
        "Title: {article[\"title\"]}\n",
        "Content: {article[\"content\"]}\n",
        "\n",
        "Provide a {treatment} summary that connects to user interests and acknowledges past coverage when relevant.\"\"\",\n",
        "                    },\n",
        "                }\n",
        "            ],\n",
        "            temperature=0.3,\n",
        "            max_tokens=500 if treatment == \"brief\" else 1000,\n",
        "        )\n",
        "\n",
        "        # Direct access to text from sampling result\n",
        "        summary = sample_result.text\n",
        "\n",
        "        # Auto-enhance if requested\n",
        "        pull_quote = \"\"\n",
        "        key_points = []\n",
        "\n",
        "        if auto_enhance:\n",
        "            await ctx.info(\"‚ú® Extracting pull quote and key points...\")\n",
        "\n",
        "            # Extract pull quote\n",
        "            quote_result = await ctx.sample(\n",
        "                messages=[\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": {\n",
        "                            \"type\": \"text\",\n",
        "                            \"text\": f\"Extract the single most impactful quote from this article (15-30 words):\\n\\n{article['content'][:2000]}\",\n",
        "                        },\n",
        "                    }\n",
        "                ],\n",
        "                temperature=0.2,\n",
        "                max_tokens=100,\n",
        "            )\n",
        "            # Direct access to text from sampling result\n",
        "            pull_quote = quote_result.text\n",
        "\n",
        "            # Extract key points\n",
        "            points_result = await ctx.sample(\n",
        "                messages=[\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": {\n",
        "                            \"type\": \"text\",\n",
        "                            \"text\": f\"Extract 3-5 key points from this article as a bullet list:\\n\\n{article['content'][:2000]}\",\n",
        "                        },\n",
        "                    }\n",
        "                ],\n",
        "                temperature=0.2,\n",
        "                max_tokens=300,\n",
        "            )\n",
        "            # Direct access to text from sampling result\n",
        "            points_text = points_result.text\n",
        "            key_points = [\n",
        "                line.strip(\"- \").strip()\n",
        "                for line in points_text.split(\"\\n\")\n",
        "                if line.strip().startswith(\"-\")\n",
        "            ]\n",
        "\n",
        "        # Add to newspaper\n",
        "        newspaper_service.add_article(\n",
        "            newspaper_id=newspaper_id,\n",
        "            section_title=section,\n",
        "            article_data={\n",
        "                \"title\": article[\"title\"],\n",
        "                \"content\": summary,\n",
        "                \"url\": article[\"url\"],\n",
        "                \"source\": article[\"source\"],\n",
        "                \"tags\": article.get(\"topics\", []),\n",
        "            },\n",
        "            placement=\"lead\" if i == 0 else \"standard\",\n",
        "        )\n",
        "\n",
        "        # Apply formatting\n",
        "        newspaper_service.set_article_format(\n",
        "            newspaper_id=newspaper_id,\n",
        "            section_title=section,\n",
        "            article_title=article[\"title\"],\n",
        "            format_options={\"pull_quote\": pull_quote, \"key_points\": key_points},\n",
        "        )\n",
        "\n",
        "        added_articles.append(article[\"title\"])\n",
        "        total_reading_time += article.get(\"reading_time\", 1)\n",
        "\n",
        "        await ctx.report_progress(progress=i + 1, total=len(articles))\n",
        "\n",
        "    # Link related if requested\n",
        "    if link_related and len(added_articles) > 1:\n",
        "        for i, title in enumerate(added_articles):\n",
        "            related_titles = [t for j, t in enumerate(added_articles) if j != i]\n",
        "            newspaper_service.link_related_articles(\n",
        "                newspaper_id=newspaper_id,\n",
        "                article_title=title,\n",
        "                related_titles=related_titles[:3],  # Max 3 links\n",
        "            )\n",
        "\n",
        "    await ctx.info(f\"‚úÖ Added {len(added_articles)} articles to '{section}'\")\n",
        "\n",
        "    result = \"# ‚úÖ Content Cluster Added\\n\\n\"\n",
        "    result += f\"**Section:** {section}\\n\"\n",
        "    result += f\"**Articles Added:** {len(added_articles)}\\n\"\n",
        "    result += f\"**Reading Time Added:** ~{total_reading_time} minutes\\n\"\n",
        "    result += f\"**Treatment:** {treatment}\\n\\n\"\n",
        "\n",
        "    result += \"**Articles:**\\n\"\n",
        "    for title in added_articles:\n",
        "        result += f\"- {title}\\n\"\n",
        "\n",
        "    if auto_enhance:\n",
        "        result += \"\\n**Enhancements Applied:**\\n\"\n",
        "        result += \"- Pull quotes extracted\\n\"\n",
        "        result += \"- Key points identified\\n\"\n",
        "\n",
        "    if link_related:\n",
        "        result += \"- Related articles cross-referenced\\n\"\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "@mcp.tool()\n",
        "async def create_editorial_synthesis(\n",
        "    newspaper_id: str,\n",
        "    content_ids: List[str],\n",
        "    angle: str = \"analytical\",\n",
        "    placement: str = \"section_intro\",\n",
        "    ctx: Context = None,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Generate editorial content connecting multiple stories using sampling.\n",
        "\n",
        "    Uses LLM to synthesize insights across articles with full context from\n",
        "    interests and past coverage.\n",
        "\n",
        "    Args:\n",
        "        newspaper_id: Target newspaper\n",
        "        content_ids: Stories to synthesize\n",
        "        angle: Editorial perspective\n",
        "               - \"analytical\": Objective analysis with implications\n",
        "               - \"educational\": Explain concepts with context\n",
        "               - \"skeptical\": Question assumptions, highlight concerns\n",
        "               - \"forward-looking\": Future trends and predictions\n",
        "        placement: Where to place\n",
        "                   - \"section_intro\": Introduction to section\n",
        "                   - \"theme_bridge\": Connect different topics\n",
        "                   - \"closing_thoughts\": Wrap-up perspective\n",
        "\n",
        "    Returns:\n",
        "        Confirmation of editorial added\n",
        "    \"\"\"\n",
        "    article_memory = ctx.request_context.lifespan_context.article_memory\n",
        "    newspaper_service = ctx.request_context.lifespan_context.newspaper_service\n",
        "    interests_service = ctx.request_context.lifespan_context.interests_service\n",
        "\n",
        "    await ctx.info(f\"‚úçÔ∏è Generating {angle} editorial...\")\n",
        "\n",
        "    # Fetch articles\n",
        "    articles = []\n",
        "    for content_id in content_ids:\n",
        "        article = article_memory.get_by_content_id(content_id)\n",
        "        if article:\n",
        "            articles.append(article)\n",
        "\n",
        "    if not articles:\n",
        "        return \"‚ùå No valid articles found\"\n",
        "\n",
        "    # Read interests\n",
        "    interests = interests_service.read_interests()\n",
        "\n",
        "    # Prepare context\n",
        "    articles_summary = \"\\n\\n\".join(\n",
        "        [f\"Article: {a['title']}\\nContent: {a['content'][:500]}...\" for a in articles]\n",
        "    )\n",
        "\n",
        "    # Generate editorial\n",
        "    await ctx.info(\"ü§ñ Generating editorial with LLM...\")\n",
        "\n",
        "    angle_instructions = {\n",
        "        \"analytical\": \"Provide objective analysis with insights and implications\",\n",
        "        \"educational\": \"Explain concepts clearly with context for learning\",\n",
        "        \"skeptical\": \"Question assumptions and highlight potential concerns\",\n",
        "        \"forward-looking\": \"Identify future trends and make predictions\",\n",
        "    }\n",
        "\n",
        "    sample_result = await ctx.sample(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": f\"\"\"Write a {angle} editorial connecting these stories (300-400 words).\n",
        "\n",
        "{angle_instructions[angle]}\n",
        "\n",
        "USER INTERESTS:\n",
        "{chr(10).join(f\"- {topic}\" for topic in interests.get(\"topics\", []))}\n",
        "\n",
        "STORIES TO CONNECT:\n",
        "{articles_summary}\n",
        "\n",
        "Write an editorial that synthesizes these stories into a coherent narrative.\"\"\",\n",
        "                },\n",
        "            }\n",
        "        ],\n",
        "        temperature=0.7,\n",
        "        max_tokens=600,\n",
        "    )\n",
        "\n",
        "    # Direct access to text from sampling result\n",
        "    editorial = sample_result.text\n",
        "\n",
        "    # Add to newspaper\n",
        "    newspaper_service.add_editors_note(\n",
        "        newspaper_id=newspaper_id,\n",
        "        content=editorial,\n",
        "        placement=placement,\n",
        "        style=\"highlighted\",\n",
        "    )\n",
        "\n",
        "    await ctx.info(f\"‚úÖ Editorial added with {angle} perspective\")\n",
        "\n",
        "    return f\"‚úÖ Editorial synthesis added\\n\\n**Angle:** {angle}\\n**Placement:** {placement}\\n**Stories Connected:** {len(articles)}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "877c1ecf",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# TOOLS: POLISH & EDITORIAL CONTROL\n",
        "# ============================================================================\n",
        "\n",
        "\n",
        "@mcp.tool()\n",
        "async def set_section_style(\n",
        "    newspaper_id: str, section: str, layout: str = \"grid\", ctx: Context = None\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Control section visual presentation.\n",
        "\n",
        "    Args:\n",
        "        newspaper_id: Target newspaper\n",
        "        section: Section to style\n",
        "        layout: Layout type\n",
        "                - \"featured\": One big article, rest smaller\n",
        "                - \"grid\": Equal-sized grid layout\n",
        "                - \"timeline\": Timeline with chronological flow\n",
        "                - \"single-column\": Single column for long-form\n",
        "    \"\"\"\n",
        "    newspaper_service = ctx.request_context.lifespan_context.newspaper_service\n",
        "\n",
        "    result = newspaper_service.set_section_layout(newspaper_id, section, layout)\n",
        "\n",
        "    if result[\"success\"]:\n",
        "        return f\"‚úÖ Set '{section}' to {layout} layout\"\n",
        "    else:\n",
        "        return f\"‚ùå {result['error']}\"\n",
        "\n",
        "\n",
        "@mcp.tool()\n",
        "async def enhance_article(\n",
        "    newspaper_id: str,\n",
        "    section: str,\n",
        "    article_title: str,\n",
        "    add_pull_quote: bool = False,\n",
        "    add_key_points: bool = False,\n",
        "    ctx: Context = None,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Add polish to specific article using LLM extraction.\n",
        "\n",
        "    Args:\n",
        "        newspaper_id: Target newspaper\n",
        "        section: Section containing article\n",
        "        article_title: Article to enhance\n",
        "        add_pull_quote: Extract compelling quote\n",
        "        add_key_points: Extract key takeaways\n",
        "    \"\"\"\n",
        "    newspaper_service = ctx.request_context.lifespan_context.newspaper_service\n",
        "    article_memory = ctx.request_context.lifespan_context.article_memory\n",
        "\n",
        "    # Get article content\n",
        "    newspaper_data = newspaper_service.get_newspaper_data(newspaper_id)\n",
        "    if not newspaper_data:\n",
        "        return \"‚ùå Newspaper not found\"\n",
        "\n",
        "    # Find article\n",
        "    article = None\n",
        "    for s in newspaper_data[\"sections\"]:\n",
        "        if s[\"title\"] == section:\n",
        "            for a in s[\"articles\"]:\n",
        "                if a[\"title\"] == article_title:\n",
        "                    article = a\n",
        "                    break\n",
        "\n",
        "    if not article:\n",
        "        return \"‚ùå Article not found\"\n",
        "\n",
        "    enhancements = {}\n",
        "\n",
        "    # Extract pull quote if requested\n",
        "    if add_pull_quote:\n",
        "        await ctx.info(\"‚ú® Extracting pull quote...\")\n",
        "        quote_result = await ctx.sample(\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": {\n",
        "                        \"type\": \"text\",\n",
        "                        \"text\": f\"Extract the most compelling quote from this article (15-30 words):\\n\\n{article['content']}\",\n",
        "                    },\n",
        "                }\n",
        "            ],\n",
        "            temperature=0.2,\n",
        "            max_tokens=100,\n",
        "        )\n",
        "        enhancements[\"pull_quote\"] = quote_result.text\n",
        "\n",
        "    # Extract key points if requested\n",
        "    if add_key_points:\n",
        "        await ctx.info(\"‚ú® Extracting key points...\")\n",
        "        points_result = await ctx.sample(\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": {\n",
        "                        \"type\": \"text\",\n",
        "                        \"text\": f\"Extract 3-5 key points from this article as a bullet list:\\n\\n{article['content']}\",\n",
        "                    },\n",
        "                }\n",
        "            ],\n",
        "            temperature=0.2,\n",
        "            max_tokens=300,\n",
        "        )\n",
        "        points_text = points_result.text\n",
        "        enhancements[\"key_points\"] = [\n",
        "            line.strip(\"- \").strip()\n",
        "            for line in points_text.split(\"\\n\")\n",
        "            if line.strip().startswith(\"-\")\n",
        "        ]\n",
        "\n",
        "    # Apply enhancements\n",
        "    newspaper_service.set_article_format(\n",
        "        newspaper_id=newspaper_id,\n",
        "        section_title=section,\n",
        "        article_title=article_title,\n",
        "        format_options=enhancements,\n",
        "    )\n",
        "\n",
        "    return f\"‚úÖ Enhanced '{article_title}' with {len(enhancements)} enhancements\"\n",
        "\n",
        "\n",
        "@mcp.tool()\n",
        "async def reorder_and_emphasize(\n",
        "    newspaper_id: str,\n",
        "    section: str,\n",
        "    article_order: List[str],\n",
        "    highlights: Dict[str, str] = None,\n",
        "    ctx: Context = None,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Control narrative flow and emphasis within section.\n",
        "\n",
        "    Args:\n",
        "        newspaper_id: Target newspaper\n",
        "        section: Section to reorder\n",
        "        article_order: List of article titles in desired order\n",
        "        highlights: Dict mapping article title to highlight type\n",
        "                    - \"breaking\": Breaking news badge\n",
        "                    - \"exclusive\": Exclusive content badge\n",
        "                    - \"trending\": Trending badge\n",
        "                    - \"deep-dive\": Deep dive badge\n",
        "    \"\"\"\n",
        "    newspaper_service = ctx.request_context.lifespan_context.newspaper_service\n",
        "\n",
        "    # Get section\n",
        "    newspaper_data = newspaper_service.get_newspaper_data(newspaper_id)\n",
        "    if not newspaper_data:\n",
        "        return \"‚ùå Newspaper not found\"\n",
        "\n",
        "    section_data = None\n",
        "    for s in newspaper_data[\"sections\"]:\n",
        "        if s[\"title\"] == section:\n",
        "            section_data = s\n",
        "            break\n",
        "\n",
        "    if not section_data:\n",
        "        return f\"‚ùå Section '{section}' not found\"\n",
        "\n",
        "    # Reorder articles\n",
        "    current_articles = {a[\"title\"]: a for a in section_data[\"articles\"]}\n",
        "    reordered = []\n",
        "\n",
        "    for title in article_order:\n",
        "        if title in current_articles:\n",
        "            reordered.append(current_articles[title])\n",
        "        else:\n",
        "            return f\"‚ùå Article '{title}' not found in section\"\n",
        "\n",
        "    section_data[\"articles\"] = reordered\n",
        "\n",
        "    # Apply highlights\n",
        "    highlights = highlights or {}\n",
        "    for article_title, highlight_type in highlights.items():\n",
        "        newspaper_service.highlight_article(\n",
        "            newspaper_id=newspaper_id,\n",
        "            section_title=section,\n",
        "            article_title=article_title,\n",
        "            highlight_type=highlight_type,\n",
        "        )\n",
        "\n",
        "    # Save changes\n",
        "    newspaper_service._save_draft(newspaper_id, newspaper_data)\n",
        "\n",
        "    return f\"‚úÖ Reordered {len(article_order)} articles in '{section}' with {len(highlights)} highlights\"\n",
        "\n",
        "\n",
        "@mcp.tool()\n",
        "async def add_editorial_element(\n",
        "    newspaper_id: str,\n",
        "    element_type: str,\n",
        "    placement: str = \"top\",\n",
        "    content: str = \"\",\n",
        "    generate: bool = False,\n",
        "    generation_context: str = \"\",\n",
        "    ctx: Context = None,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Add editorial elements (notes, highlights, etc).\n",
        "\n",
        "    Can either use provided content OR generate using LLM.\n",
        "\n",
        "    Args:\n",
        "        newspaper_id: Target newspaper\n",
        "        element_type: Type of element\n",
        "                      - \"editors_note\": Editor's commentary\n",
        "                      - \"theme_highlight\": Connect cross-theme stories\n",
        "                      - \"stats_callout\": Highlight key statistics\n",
        "        placement: Where to place (\"top\", \"bottom\", or \"section:<name>\")\n",
        "        content: Content to use (if not generating)\n",
        "        generate: If True, use LLM to generate content\n",
        "        generation_context: Context for generation (e.g., \"Connect privacy and performance themes\")\n",
        "    \"\"\"\n",
        "    newspaper_service = ctx.request_context.lifespan_context.newspaper_service\n",
        "\n",
        "    # Generate content if requested\n",
        "    if generate and generation_context:\n",
        "        await ctx.info(f\"ü§ñ Generating {element_type} content...\")\n",
        "\n",
        "        newspaper_data = newspaper_service.get_newspaper_data(newspaper_id)\n",
        "\n",
        "        sample_result = await ctx.sample(\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": {\n",
        "                        \"type\": \"text\",\n",
        "                        \"text\": f\"\"\"Generate a {element_type} for a newspaper.\n",
        "\n",
        "Context: {generation_context}\n",
        "\n",
        "Newspaper Title: {newspaper_data.get(\"title\", \"\")}\n",
        "Sections: {\", \".join(s[\"title\"] for s in newspaper_data.get(\"sections\", []))}\n",
        "\n",
        "Write 2-3 sentences appropriate for a {element_type}.\"\"\",\n",
        "                    },\n",
        "                }\n",
        "            ],\n",
        "            temperature=0.6,\n",
        "            max_tokens=200,\n",
        "        )\n",
        "\n",
        "        # Direct access to text from sampling result\n",
        "        content = sample_result.text\n",
        "\n",
        "    # Add element based on type\n",
        "    if element_type == \"editors_note\":\n",
        "        newspaper_service.add_editors_note(\n",
        "            newspaper_id=newspaper_id,\n",
        "            content=content,\n",
        "            placement=placement,\n",
        "            style=\"highlighted\",\n",
        "        )\n",
        "    elif element_type == \"theme_highlight\":\n",
        "        newspaper_service.add_theme_highlight(\n",
        "            newspaper_id=newspaper_id,\n",
        "            theme=generation_context.split()[0] if generation_context else \"Theme\",\n",
        "            description=content,\n",
        "            related_articles=[],\n",
        "        )\n",
        "    else:\n",
        "        return f\"‚ùå Unsupported element type: {element_type}\"\n",
        "\n",
        "    return f\"‚úÖ Added {element_type} at {placement}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "4d9af0da",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# TOOLS: QUALITY CONTROL & DELIVERY\n",
        "# ============================================================================\n",
        "\n",
        "\n",
        "@mcp.tool()\n",
        "async def preview_newspaper(\n",
        "    newspaper_id: str, preview_type: str = \"summary\", ctx: Context = None\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Preview newspaper with different analysis views.\n",
        "\n",
        "    Args:\n",
        "        newspaper_id: Newspaper to preview\n",
        "        preview_type: Type of preview\n",
        "                      - \"summary\": Stats and metadata\n",
        "                      - \"structure\": Section breakdown\n",
        "                      - \"full\": Complete markdown\n",
        "    \"\"\"\n",
        "    newspaper_service = ctx.request_context.lifespan_context.newspaper_service\n",
        "\n",
        "    if preview_type in [\"summary\", \"structure\"]:\n",
        "        result = newspaper_service.get_stats(newspaper_id)\n",
        "        if not result[\"success\"]:\n",
        "            return f\"‚ùå {result['error']}\"\n",
        "\n",
        "        stats = result[\"stats\"]\n",
        "        output = \"# üìä Newspaper Summary\\n\\n\"\n",
        "        output += f\"**Title:** {stats['title']}\\n\"\n",
        "        output += f\"**Type:** {stats['edition_type']}\\n\"\n",
        "        output += f\"**Articles:** {stats['total_articles']}\\n\"\n",
        "        output += f\"**Reading Time:** {stats['total_reading_time']} minutes\\n\"\n",
        "        output += f\"**Sections:** {stats['section_count']}\\n\\n\"\n",
        "\n",
        "        output += \"**Section Breakdown:**\\n\"\n",
        "        for section in stats[\"sections\"]:\n",
        "            output += f\"- {section['title']} ({section['layout']}): {section['article_count']} articles\\n\"\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "@mcp.tool()\n",
        "async def validate_and_finalize(\n",
        "    newspaper_id: str,\n",
        "    min_reading_time: int = None,\n",
        "    min_articles: int = None,\n",
        "    ctx: Context = None,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Validate newspaper meets quality standards with ENFORCEMENT.\n",
        "\n",
        "    Unlike old design, this PREVENTS publication if standards not met\n",
        "    and provides actionable fixes.\n",
        "\n",
        "    Args:\n",
        "        newspaper_id: Newspaper to validate\n",
        "        min_reading_time: Minimum reading time in minutes (optional)\n",
        "        min_articles: Minimum number of articles (optional)\n",
        "\n",
        "    Returns:\n",
        "        Validation results with actionable fixes if needed\n",
        "    \"\"\"\n",
        "    newspaper_service = ctx.request_context.lifespan_context.newspaper_service\n",
        "\n",
        "    await ctx.info(\"üîç Validating newspaper quality...\")\n",
        "\n",
        "    # Get newspaper data\n",
        "    newspaper_data = newspaper_service.get_newspaper_data(newspaper_id)\n",
        "    if not newspaper_data:\n",
        "        return \"‚ùå Newspaper not found\"\n",
        "\n",
        "    # Check basic validation\n",
        "    result = newspaper_service.validate(newspaper_id)\n",
        "\n",
        "    issues = []\n",
        "    warnings = []\n",
        "\n",
        "    # Check custom requirements\n",
        "    current_reading_time = newspaper_data[\"metadata\"][\"total_reading_time\"]\n",
        "    current_articles = newspaper_data[\"metadata\"][\"article_count\"]\n",
        "\n",
        "    if min_reading_time and current_reading_time < min_reading_time:\n",
        "        shortfall = min_reading_time - current_reading_time\n",
        "        issues.append(\n",
        "            {\n",
        "                \"type\": \"reading_time\",\n",
        "                \"current\": current_reading_time,\n",
        "                \"required\": min_reading_time,\n",
        "                \"suggestion\": f\"Add {shortfall // 3} more detailed articles (~3min each)\",\n",
        "                \"fix\": f\"Use add_content_cluster() with {shortfall // 3} content IDs and treatment='detailed'\",\n",
        "            }\n",
        "        )\n",
        "\n",
        "    if min_articles and current_articles < min_articles:\n",
        "        shortfall = min_articles - current_articles\n",
        "        issues.append(\n",
        "            {\n",
        "                \"type\": \"article_count\",\n",
        "                \"current\": current_articles,\n",
        "                \"required\": min_articles,\n",
        "                \"suggestion\": f\"Add {shortfall} more articles\",\n",
        "                \"fix\": f\"Use add_content_cluster() with {shortfall} content IDs\",\n",
        "            }\n",
        "        )\n",
        "\n",
        "    # Check for empty sections\n",
        "    for section in newspaper_data[\"sections\"]:\n",
        "        if not section[\"articles\"]:\n",
        "            warnings.append(f\"Section '{section['title']}' is empty\")\n",
        "\n",
        "    # Combine with basic validation\n",
        "    all_issues = result.get(\"issues\", []) + [i[\"type\"] for i in issues]\n",
        "    all_warnings = result.get(\"warnings\", []) + warnings\n",
        "\n",
        "    # Format output\n",
        "    if not all_issues:\n",
        "        output = \"# ‚úÖ Newspaper Valid!\\n\\n\"\n",
        "        output += \"All quality standards met. Ready to publish.\\n\\n\"\n",
        "\n",
        "        if all_warnings:\n",
        "            output += \"**Warnings:**\\n\"\n",
        "            for warning in all_warnings:\n",
        "                output += f\"‚ö†Ô∏è {warning}\\n\"\n",
        "\n",
        "        return output\n",
        "    else:\n",
        "        output = \"# ‚ùå Newspaper Needs Improvement\\n\\n\"\n",
        "\n",
        "        output += \"**Issues Found:**\\n\"\n",
        "        for issue in issues:\n",
        "            output += f\"\\n**{issue['type'].replace('_', ' ').title()}**\\n\"\n",
        "            output += f\"- Current: {issue['current']}\\n\"\n",
        "            output += f\"- Required: {issue['required']}\\n\"\n",
        "            output += f\"- Suggestion: {issue['suggestion']}\\n\"\n",
        "            output += f\"- Fix: `{issue['fix']}`\\n\"\n",
        "\n",
        "        if all_warnings:\n",
        "            output += \"\\n**Warnings:**\\n\"\n",
        "            for warning in all_warnings:\n",
        "                output += f\"‚ö†Ô∏è {warning}\\n\"\n",
        "\n",
        "        output += \"\\n**Cannot publish until issues resolved.**\\n\"\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "@mcp.tool()\n",
        "async def publish_newspaper(\n",
        "    newspaper_id: str, delivery_method: str = \"email\", ctx: Context = None\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Finalize and deliver newspaper.\n",
        "\n",
        "    Args:\n",
        "        newspaper_id: Newspaper to publish\n",
        "        delivery_method: How to deliver\n",
        "                         - \"email\": Send via email\n",
        "                         - \"save_html\": Save HTML locally\n",
        "                         - \"both\": Email and save\n",
        "    \"\"\"\n",
        "    newspaper_service = ctx.request_context.lifespan_context.newspaper_service\n",
        "    email_service = ctx.request_context.lifespan_context.email_service\n",
        "    article_memory = ctx.request_context.lifespan_context.article_memory\n",
        "    settings = ctx.request_context.lifespan_context.settings\n",
        "\n",
        "    await ctx.info(\"üì∞ Publishing newspaper...\")\n",
        "\n",
        "    # Get newspaper data\n",
        "    newspaper_data = newspaper_service.get_newspaper_data(newspaper_id)\n",
        "    if not newspaper_data:\n",
        "        return \"‚ùå Newspaper not found\"\n",
        "\n",
        "    # Generate HTML\n",
        "    await ctx.report_progress(progress=0, total=3)\n",
        "    html_content = email_service._create_html_version(newspaper_data)\n",
        "\n",
        "    # Save HTML\n",
        "    html_file = settings.data_dir / \"newspapers\" / f\"{newspaper_id}.html\"\n",
        "    html_file.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with open(html_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(html_content)\n",
        "\n",
        "    await ctx.report_progress(progress=1, total=3)\n",
        "\n",
        "    # Send email if requested\n",
        "    email_sent = False\n",
        "    if delivery_method in [\"email\", \"both\"]:\n",
        "        result = email_service.send_newspaper(newspaper_data, version=2)\n",
        "        email_sent = result[\"success\"]\n",
        "        if not email_sent:\n",
        "            await ctx.warning(f\"Email failed: {result.get('error', 'Unknown error')}\")\n",
        "\n",
        "    await ctx.report_progress(progress=2, total=3)\n",
        "\n",
        "    # Store in archive\n",
        "    article_memory.store_newspaper(newspaper_id, newspaper_data)\n",
        "\n",
        "    await ctx.report_progress(progress=3, total=3)\n",
        "    await ctx.info(\"‚úÖ Newspaper published and archived\")\n",
        "\n",
        "    # Format output\n",
        "    output = \"# ‚úÖ Newspaper Published!\\n\\n\"\n",
        "    output += f\"**Title:** {newspaper_data['title']}\\n\"\n",
        "    output += f\"**Articles:** {newspaper_data['metadata']['article_count']}\\n\"\n",
        "    output += f\"**Reading Time:** {newspaper_data['metadata']['total_reading_time']} minutes\\n\\n\"\n",
        "\n",
        "    if email_sent:\n",
        "        output += \"üìß **Email:** Sent successfully\\n\"\n",
        "\n",
        "    if delivery_method in [\"save_html\", \"both\"] or not email_sent:\n",
        "        output += f\"üíæ **HTML:** Saved to {html_file}\\n\"\n",
        "\n",
        "    output += \"üóÑÔ∏è **Archive:** Stored in memory\\n\"\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "8dd29a56",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# TOOLS: CONTEXT HELPERS\n",
        "# ============================================================================\n",
        "\n",
        "\n",
        "@mcp.tool()\n",
        "async def search_context(\n",
        "    query: str, context_type: str = \"articles\", limit: int = 10, ctx: Context = None\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Search archive for relevant context.\n",
        "\n",
        "    Args:\n",
        "        query: Search query\n",
        "        context_type: What to search\n",
        "                      - \"articles\": Search article archive\n",
        "                      - \"newspapers\": Search past newspapers\n",
        "                      - \"topics\": Search by topic\n",
        "        limit: Maximum results (1-20)\n",
        "    \"\"\"\n",
        "    article_memory = ctx.request_context.lifespan_context.article_memory\n",
        "\n",
        "    if context_type == \"articles\":\n",
        "        articles = article_memory.search_articles(query=query, limit=limit)\n",
        "\n",
        "        if not articles:\n",
        "            return f\"No articles found for '{query}'\"\n",
        "\n",
        "        result = f\"# üîç Found {len(articles)} Articles\\n\\n\"\n",
        "        for i, article in enumerate(articles, 1):\n",
        "            result += f\"## {i}. {article['title']}\\n\"\n",
        "            result += f\"**Content ID:** {article.get('content_id', 'unknown')}\\n\"\n",
        "            result += f\"**Similarity:** {article['similarity']:.1%}\\n\"\n",
        "            result += f\"**Source:** {article['source']}\\n\\n\"\n",
        "\n",
        "        return result\n",
        "\n",
        "    elif context_type == \"newspapers\":\n",
        "        newspapers = article_memory.search_newspapers(days_back=90, query=query)\n",
        "\n",
        "        if not newspapers:\n",
        "            return f\"No newspapers found for '{query}'\"\n",
        "\n",
        "        result = f\"# üîç Found {len(newspapers)} Newspapers\\n\\n\"\n",
        "        for paper in newspapers:\n",
        "            result += f\"## {paper['title']}\\n\"\n",
        "            result += f\"**Date:** {paper['timestamp'][:10]}\\n\"\n",
        "            result += f\"**Type:** {paper['edition_type']}\\n\"\n",
        "            result += f\"**Articles:** {paper['article_count']}\\n\\n\"\n",
        "\n",
        "        return result\n",
        "\n",
        "    else:\n",
        "        return f\"‚ùå Unsupported context type: {context_type}\"\n",
        "\n",
        "\n",
        "@mcp.tool()\n",
        "async def get_related_content(\n",
        "    content_id: str,\n",
        "    relationship_type: str = \"similar\",\n",
        "    limit: int = 5,\n",
        "    ctx: Context = None,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Find content related to a specific article.\n",
        "\n",
        "    Args:\n",
        "        content_id: Content ID to find relations for\n",
        "        relationship_type: Type of relationship\n",
        "                           - \"similar\": Similar content\n",
        "                           - \"follow_up\": Follow-up coverage\n",
        "                           - \"background\": Background context\n",
        "        limit: Maximum results (1-20)\n",
        "    \"\"\"\n",
        "    article_memory = ctx.request_context.lifespan_context.article_memory\n",
        "\n",
        "    # Get base article\n",
        "    article = article_memory.get_by_content_id(content_id)\n",
        "    if not article:\n",
        "        return f\"‚ùå Content ID not found: {content_id}\"\n",
        "\n",
        "    # Search for related\n",
        "    related = article_memory.search_articles(query=article[\"content\"], limit=limit)\n",
        "\n",
        "    # Filter out the original article\n",
        "    related = [r for r in related if r.get(\"content_id\") != content_id]\n",
        "\n",
        "    if not related:\n",
        "        return f\"No related content found for '{article['title']}'\"\n",
        "\n",
        "    result = f\"# üîó Related to: {article['title']}\\n\\n\"\n",
        "    result += f\"**Relationship Type:** {relationship_type}\\n\"\n",
        "    result += f\"**Found:** {len(related)} related articles\\n\\n\"\n",
        "\n",
        "    for i, rel in enumerate(related, 1):\n",
        "        result += f\"## {i}. {rel['title']}\\n\"\n",
        "        result += f\"**Content ID:** {rel.get('content_id', 'unknown')}\\n\"\n",
        "        result += f\"**Similarity:** {rel['similarity']:.1%}\\n\"\n",
        "        result += f\"**Source:** {rel['source']}\\n\\n\"\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "004c56d8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PROMPTS\n",
        "# ============================================================================\n",
        "\n",
        "\n",
        "@mcp.prompt()\n",
        "async def create_morning_brief() -> str:\n",
        "    \"\"\"Workflow for creating a quick morning newspaper (15-20min read).\"\"\"\n",
        "    return \"\"\"Create a focused morning tech brief (15-20min read)\n",
        "\n",
        "WORKFLOW:\n",
        "1. discover_stories(query=\"tech news today\", count=20, sources=[\"hn\"])\n",
        "   ‚Üí Review summaries, pick top stories\n",
        "\n",
        "2. create_newspaper(type=\"morning_brief\", title=f\"Morning Brief - {date}\")\n",
        "\n",
        "3. add_content_cluster(\n",
        "     newspaper_id,\n",
        "     section=\"Breaking News\",\n",
        "     content_ids=[top 3 content_ids],\n",
        "     treatment=\"brief\",\n",
        "     auto_enhance=True\n",
        "   )\n",
        "\n",
        "4. add_content_cluster(\n",
        "     newspaper_id,\n",
        "     section=\"Quick Reads\",\n",
        "     content_ids=[next 4-5 content_ids],\n",
        "     treatment=\"brief\"\n",
        "   )\n",
        "\n",
        "5. Optional: create_editorial_synthesis if themes emerge\n",
        "\n",
        "6. validate_and_finalize(\n",
        "     newspaper_id,\n",
        "     min_reading_time=15,\n",
        "     min_articles=5\n",
        "   )\n",
        "\n",
        "7. publish_newspaper(delivery_method=\"email\")\n",
        "\n",
        "TOOL CALLS: ~8-10\n",
        "AGENT CONTROLS: Which stories, editorial synthesis decision, final polish\"\"\"\n",
        "\n",
        "\n",
        "@mcp.prompt()\n",
        "async def create_deep_dive() -> str:\n",
        "    \"\"\"Workflow for comprehensive deep dive newspaper (30-45min read).\"\"\"\n",
        "    return \"\"\"Create comprehensive deep dive newspaper (30-45min read)\n",
        "\n",
        "WORKFLOW:\n",
        "1. discover_stories(query=\"tech\", count=30, sources=[\"hn\"])\n",
        "   ‚Üí Review enriched summaries with relevance scores\n",
        "\n",
        "2. Review resource: memory://context-summary\n",
        "   ‚Üí Check trending topics and coverage gaps\n",
        "\n",
        "3. Group stories by theme (use your judgment based on topics)\n",
        "\n",
        "4. create_newspaper(type=\"deep_dive\", title=\"Deep Dive: {themes}\")\n",
        "\n",
        "5. For each theme (3-4 themes):\n",
        "   a. add_content_cluster(\n",
        "        newspaper_id,\n",
        "        section=theme_name,\n",
        "        content_ids=[3-4 content_ids],\n",
        "        treatment=\"detailed\",\n",
        "        auto_enhance=True,\n",
        "        link_related=True\n",
        "      )\n",
        "      ‚Üí Tool handles: fetch, context, sampling, formatting\n",
        "\n",
        "   b. enhance_article(\n",
        "        newspaper_id,\n",
        "        section=theme_name,\n",
        "        article_title=lead_article,\n",
        "        add_pull_quote=True,\n",
        "        add_key_points=True\n",
        "      )\n",
        "      ‚Üí Extra polish for lead article\n",
        "\n",
        "   c. create_editorial_synthesis(\n",
        "        newspaper_id,\n",
        "        content_ids=theme_content_ids,\n",
        "        angle=\"analytical\",\n",
        "        placement=\"section_intro\"\n",
        "      )\n",
        "\n",
        "6. If themes connect:\n",
        "   add_editorial_element(\n",
        "     newspaper_id,\n",
        "     element_type=\"theme_highlight\",\n",
        "     generate=True,\n",
        "     generation_context=\"Connect cross-theme patterns\"\n",
        "   )\n",
        "\n",
        "7. validate_and_finalize(\n",
        "     newspaper_id,\n",
        "     min_reading_time=30,\n",
        "     min_articles=8\n",
        "   )\n",
        "   ‚Üí If fails: Get specific fixes, execute, re-validate\n",
        "\n",
        "8. publish_newspaper(delivery_method=\"both\")\n",
        "\n",
        "TOOL CALLS: ~20-25\n",
        "AGENT CONTROLS: Theme selection, lead articles, editorial angles, cross-theme synthesis\n",
        "DELEGATED: Content fetching, summarization, formatting, context inclusion\"\"\"\n",
        "\n",
        "\n",
        "@mcp.prompt()\n",
        "async def follow_story() -> str:\n",
        "    \"\"\"Follow up on story from past newspapers.\"\"\"\n",
        "    return \"\"\"Follow up on story from past newspapers\n",
        "\n",
        "WORKFLOW:\n",
        "1. search_context(query=topic, context_type=\"newspapers\")\n",
        "   ‚Üí Identify past coverage\n",
        "\n",
        "2. Read resource: memory://articles/{topic}\n",
        "   ‚Üí See all related past articles\n",
        "\n",
        "3. discover_stories(query=topic, count=15, sources=[\"hn\"])\n",
        "   ‚Üí Find new developments\n",
        "\n",
        "4. get_related_content(content_id=original_story, relationship_type=\"follow_up\")\n",
        "\n",
        "5. create_newspaper(\n",
        "     type=\"follow_up\",\n",
        "     title=f\"Follow-up: {topic}\",\n",
        "     structure_template=past_newspaper_id\n",
        "   )\n",
        "\n",
        "6. add_content_cluster(\n",
        "     newspaper_id,\n",
        "     section=\"What Changed\",\n",
        "     content_ids=new_developments,\n",
        "     treatment=\"detailed\"\n",
        "   )\n",
        "\n",
        "7. add_content_cluster(\n",
        "     newspaper_id,\n",
        "     section=\"Deep Analysis\",\n",
        "     content_ids=analysis_pieces,\n",
        "     treatment=\"technical\"\n",
        "   )\n",
        "\n",
        "8. create_editorial_synthesis(\n",
        "     newspaper_id,\n",
        "     content_ids=[all],\n",
        "     angle=\"forward-looking\",\n",
        "     placement=\"closing_thoughts\"\n",
        "   )\n",
        "\n",
        "9. validate_and_finalize + publish_newspaper\n",
        "\n",
        "TOOL CALLS: ~15-20\n",
        "AGENT CONTROLS: What's \"new\" vs rehash, comparative emphasis, predictions\"\"\"\n",
        "\n",
        "\n",
        "@mcp.prompt()\n",
        "async def interactive_research() -> str:\n",
        "    \"\"\"Interactive research with continuous user input.\"\"\"\n",
        "    return \"\"\"Interactive research session with elicitation\n",
        "\n",
        "WORKFLOW:\n",
        "1. Get topic from context or ask user\n",
        "\n",
        "2. search_context(query=topic) + memory://articles/{topic}\n",
        "   ‚Üí \"Here's what I already know\"\n",
        "\n",
        "3. discover_stories(query=topic, count=20)\n",
        "   ‚Üí Agent reviews, proposes focus areas\n",
        "\n",
        "4. Use elicitation in add_content_cluster for depth decisions\n",
        "\n",
        "5. For each focus area:\n",
        "   a. add_content_cluster (builds newspaper progressively)\n",
        "   b. Check if user wants to pivot or go deeper\n",
        "   c. get_related_content to expand coverage\n",
        "\n",
        "6. create_newspaper when sufficient coverage\n",
        "   a. Agent proposes structure based on exploration\n",
        "   b. Add all explored content with appropriate treatments\n",
        "   c. create_editorial_synthesis with chosen angle\n",
        "\n",
        "7. validate_and_finalize + publish_newspaper\n",
        "\n",
        "TOOL CALLS: Variable (user-driven)\n",
        "AGENT CONTROLS: Information synthesis, structure proposals\n",
        "USER CONTROLS (via elicitation): Direction, depth, when to finalize\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da369b36",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# SERVER STARTUP\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import warnings\n",
        "\n",
        "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"üöÄ ADVANCED NEWSPAPER AGENT MCP SERVER\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"\\nüìä Server Statistics:\")\n",
        "    print(\n",
        "        \"  ‚Ä¢ Resources: 4 (interests, context summary, articles by topic, recent newspapers)\"\n",
        "    )\n",
        "    print(\"  ‚Ä¢ Tools: 14 (discovery, creation, polish, quality, context)\")\n",
        "    print(\"  ‚Ä¢ Prompts: 4 (morning brief, deep dive, follow story, research)\")\n",
        "    print(\"\\n‚ú® Key Features:\")\n",
        "    print(\"  ‚Ä¢ Smart composition - one tool does many operations\")\n",
        "    print(\"  ‚Ä¢ Content IDs - clean references for agents\")\n",
        "    print(\"  ‚Ä¢ Automatic context - interests & past coverage included\")\n",
        "    print(\"  ‚Ä¢ Sampling - LLM generation for summaries & editorials\")\n",
        "    print(\"  ‚Ä¢ Elicitation - interactive user choices\")\n",
        "    print(\"  ‚Ä¢ Progress - real-time operation updates\")\n",
        "    print(\"  ‚Ä¢ Quality enforcement - prevents low-quality newspapers\")\n",
        "    print(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
        "\n",
        "    # Kill any existing process on port 8080\n",
        "    os.system(\"lsof -ti:8080 | xargs kill -9 2>/dev/null\")\n",
        "\n",
        "    # Run server\n",
        "    asyncio.run(mcp.run_async(transport=\"streamable-http\", port=8080))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a75d8e8a",
      "metadata": {},
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "## Checkpoint 6 Results\n",
        "\n",
        "<img src=\"https://github.com/adityaarunsinghal/agentic-ai-workshop-2025/blob/main/notebooks/media/session02checkpoint06_newspaper.png?raw=true\" width=\"1500\" alt=\"session02checkpoint06_newspaper\">\n",
        "<!-- <img src=\"media/session02checkpoint06_newspaper.png\" width=\"1500\" alt=\"session02checkpoint06_newspaper\"> -->\n",
        "\n",
        "### Great newspaper!\n",
        "\n",
        "#### [üìÑ Open Newspaper in Browser](https://htmlpreview.github.io/?https://github.com/adityaarunsinghal/agentic-ai-workshop-2025/blob/main/examples/newspaper_20251008_160145.html)\n",
        "\n",
        "<img src=\"https://github.com/adityaarunsinghal/agentic-ai-workshop-2025/blob/main/notebooks/media/session02checkpoint06_sampling_and_progress.png?raw=true\" width=\"1500\" alt=\"session02checkpoint06_sampling_and_progress\">\n",
        "<!-- <img src=\"media/session02checkpoint06_sampling_and_progress.png\" width=\"1500\" alt=\"session02checkpoint06_sampling_and_progress\"> -->\n",
        "\n",
        "### Very few tokens held in memory\n",
        "\n",
        "<br/>\n",
        "<img src=\"https://github.com/adityaarunsinghal/agentic-ai-workshop-2025/blob/main/notebooks/media/session02checkpoint06_low_tokens_held.png?raw=true\" width=\"1500\" alt=\"session02checkpoint06_low_tokens_held\">\n",
        "<!-- <img src=\"media/session02checkpoint06_low_tokens_held.png\" width=\"1500\" alt=\"session02checkpoint06_low_tokens_held\"> -->\n",
        "\n",
        "### \"Smart tools + Sampling\"\n",
        "\n",
        "_Mainline agent is not carrying around too many tokens but still has all the power_\n",
        "\n",
        "\n",
        "</div>"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
